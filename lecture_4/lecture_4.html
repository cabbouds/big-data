<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 4</title>
<meta name="author" content="(Adolfo De Unánue)"/>

<link rel="stylesheet" href="http://cdn.jsdelivr.net/reveal.js/2.5.0/css/reveal.css"/>
<link rel="stylesheet" href="http://cdn.jsdelivr.net/reveal.js/2.5.0/css/theme/night.css" id="theme"/>
<link rel="stylesheet" href="itam-org-reveal.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'http://cdn.jsdelivr.net/reveal.js/2.5.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro' rel='stylesheet' type='text/css'>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1>Lecture 4</h1>
<h2>Adolfo De Unánue</h2>
<h2><a href="mailto:adolfo.deunanue@itam.mx">adolfo.deunanue@itam.mx</a></h2>
<h2></h2>
</section>

<section>
<section id="slide-sec-1" data-background="#000fff">
<h2 id="sec-1">Docker</h2>

</section>
<section id="slide-sec-1-1">
<h3 id="sec-1-1">Obtener la imagen</h3>
<pre><code data-trim>

docker pull nanounanue/docker-hadoop
</code></pre>


</section>
<section id="slide-sec-1-2">
<h3 id="sec-1-2">Ejecutar un contenedor</h3>
<pre><code data-trim>


docker run -ti --rm \
  -v /ruta/a/tus/datos/:/home/itam/data \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 \
nanounanue/docker-hadoop /bin/zsh
</code></pre>


</section>
<section id="slide-sec-1-3">
<h3 id="sec-1-3">Contenedor con Hadoop</h3>
<pre><code data-trim>

docker run -ti --name hadoop-pseudo \
  -v /ruta/a/tus/datos/:/home/itam/data \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 -p 9999:9999 \
nanounanue/docker_hadoop
</code></pre>


</section>
<section id="slide-sec-1-4">
<h3 id="sec-1-4">Reiniciar el contenedor</h3>
<pre><code data-trim>

docker start -ai hadoop-pseudo
</code></pre>


</section>
<section id="slide-sec-1-5">
<h3 id="sec-1-5">Conectarse a un contenedor funcionando</h3>
<ul>
<li>Esto podría ser útil para ver, por ejemplo <code>logs</code> o arrancar varios clientes.</li>

<li>Averigua el número del contenedor</li>

</ul>

<pre><code data-trim>

docker ps -a
</code></pre>



<pre><code data-trim>
docker exec -it <CONTENEDOR_ID> /bin/zsh
</code></pre>


<p>
o en nuestro caso:
</p>

<pre><code data-trim>
docker exec -it hadoop_pseudo /bin/zsh
</code></pre>


<ul>
<li><b>Nota</b>: <i>Recuerda que con los primeros 4 dígitos del contenedor basta para identificarlo.</i></li>

</ul>



</section>
<section id="slide-sec-1-6">
<h3 id="sec-1-6">Navegador Web</h3>
<ul>
<li><a href="http://127.0.0.1:50090">Consola de Yarn</a></li>

<li><a href="http://127.0.0.1:50070">Consola de HDFS</a></li>

<li><a href="http://0.0.0.0:8000">HUE - Hadoop User Experience</a>
<ul>
<li><i>Desactivado</i></li>

</ul></li>

</ul>




</section>
<section id="slide-sec-1-7">
<h3 id="sec-1-7">Ejercicio</h3>
<ul>
<li>Explicar los diferentes modos en los que puede ejecutarse <b><b>Apache Hadoop</b></b>.</li>

<li><b>Modo Pseudodistribuido</b>

<ul>
<li>Crea el contendeor de Docker para Hadoop,  vamos a explicar que significa <i>pseudodistribuido</i>.</li>

</ul></li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-2" data-background="#000fff">
<h2 id="sec-2">Apache Hadoop</h2>

</section>
<section id="slide-sec-2-1">
<h3 id="sec-2-1">¿Por qué?</h3>
<ul>
<li>Aunque la capacidad de los discos ha aumentado considerablemente, la velocidad de los mismos no lo ha hecho igual.
<ul>
<li>Los discos actuales de <code>1 Tb</code>, tardan en leerse completos a <code>100 Mb/s</code> cerca de dos a tres horas.</li>
<li>Podemos <i>paralelizar</i> las fuentes en varios discos.
<ul>
<li>Para leerla simultáneamente</li>

</ul></li>
<li>Con varios discos, la <b><b>probabilidad de falla</b></b> aumenta.</li>

</ul></li>
<li>Otro problema es la distribución ¿Cómo combinas varios <code>file systems</code>?</li>

</ul>

</section>
<section id="slide-sec-2-2">
<h3 id="sec-2-2">¿Qué es?</h3>
<ul>
<li>Sistema confiable (<i>realiable</i>) de almacenamiento compartido y de procesamiento de datos.
<ul>
<li><b>Almacenamiento</b>: <i>Hadoop Distributed File System</i>, <code>HDFS</code></li>
<li><b>Procesamiento</b>: Varios <i>frameworks</i> basados en <code>YARN</code>.</li>

</ul></li>

<li>Puede procesar cantidades masivas de datos y escalar conforme crezcan los datos.</li>

<li>Flexibilidad para el procesamiento de datos.
<ul>
<li>No importa la estructura o falta de ella</li>

</ul></li>

<li>Está construido en <code>Java</code>.</li>

</ul>

</section>
<section id="slide-sec-2-3">
<h3 id="sec-2-3">¿Cómo?</h3>
<ul>
<li><code>MapReduce</code> es un sistema de procesamiento <i>batch</i>
<ul>
<li>Permite correr <i>queries</i> contra <b><b>toda</b></b> tu base de datos</li>
<li>Pero el resultado puede tardar minutos, horas, etc&#x2026;</li>
<li>No permite tener a un humano sentado ahí para retroalimentar.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-4">
<h3 id="sec-2-4">¿Cómo?</h3>
<ul>
<li>Ahora, gracias a <code>YARN</code> (ver más adelante) tenemos diferentes tipos de procesamiento:
<ul>
<li><i>SQL Interactivo</i>: <code>Impala</code>, <code>Hive</code>, <code>Spark SQL</code>.</li>
<li><i>Iterativos</i>: <code>Spark</code>.</li>
<li><i>Procesamiento de flujos</i>: <code>Storm</code>, <code>Spark Streaming</code>.</li>
<li><i>Búsquedas</i>: <code>Solr</code>.</li>
<li><i>Grafos</i> <code>Spark GraphX</code>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-5">
<h3 id="sec-2-5">¿Por qué no otros sistemas?</h3>
<ul>
<li>¿Por qué no usar un <code>PostgreSQL</code> con muchos discos, muy <i>pimpeado</i>?
<ul>
<li>El problema viene del tiempo que toma mover la cabeza del disco a otro lugar del disco para leer o escribir datos (<i>seek time</i>).
<ul>
<li>¿Cuál es la <i>latencia</i> de la operación?</li>

</ul></li>

</ul></li>

<li>¿Por qué no <i>Grid</i>?
<ul>
<li>Por ejemplo, cosas  de <code>HPC</code> que usan <code>MPI</code>.
<ul>
<li>Son intensivos en <b><b>CPU</b></b>.</li>

</ul></li>
<li>Pero si hay que mover cientos de gigabytes, la transferencia de datos se vuelve un problema.
<ul>
<li>Basicamente, en que <code>Hadoop</code> opera con <i>data locality</i>.</li>

</ul></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-6">
<h3 id="sec-2-6">Componentes de Apache Hadoop</h3>
<ul>
<li><b>MapReduce</b> Modelo de procesamiento <i>batch</i> de datos distribuido y paralelo.</li>
<li><b>HDFS</b> Sistema de archivos (<i>file system</i>) distribuido.</li>
<li><b>Pig</b> Capa de abstracción encima de <code>MapReduce</code>. Utiliza <i>Pig Latin</i> un lenguaje de flujo de datos
<ul>
<li>Como <code>dplyr</code></li>

</ul></li>
<li><b>Hive</b> (Hadoop InteractiVE) Es un lenguaje parecido al <code>SQL</code>: <code>HQL</code>, para ejecutar <i>queries</i> sobre el <code>HDFS</code>.</li>
<li><b>HBase</b> Base de datos distribuida orientada a columnas.
<ul>
<li>Depende de <code>Zookeeper</code>.</li>

</ul></li>
<li><b>Impala</b> Lenguaje Interactivo parecido al <code>SQL</code>, pero mucho más rápido de <code>HIVE</code> debido a su arquitectura <b>MPP</b>.</li>

</ul>

</section>
<section id="slide-sec-2-7">
<h3 id="sec-2-7">Componentes de Apache Hadoop</h3>
<ul>
<li><b>Zookeeper</b> Proyecto que proveé un servicio centralizado para facilitar la coordinación de componentes de Hadoop.</li>
<li><b>Sqoop</b> Herramienta para mover datos entre <code>RDBM</code> y <code>HDFS</code>.</li>
<li><b>Flume</b> Servicio para recolectar, agregar y mover grandes cantidades de datos entre máquinas individuales y el <code>HDFS</code>.</li>
<li><b>Oozie</b> Sistema de <i>workflow</i>, se usa para coordinar varios <i>jobs</i> de <b>MapReduce</b>.</li>
<li><b>Mahout</b> Biblioteca de <i>Machine Learning</i>.
<ul>
<li>Ver la carpeta <code>docs</code>.</li>

</ul></li>
<li><b>Ambari</b> Simplifica el aprovisionamiento, gestión y <i>monitoreo</i> de un <i>cluster</i> de Hadoop.</li>
<li><b>Avro</b> Formato de serialización y de persistencia de datos.</li>
<li>Entre otros&#x2026;</li>

</ul>



</section>
</section>
<section>
<section id="slide-sec-3" data-background="#000fff">
<h2 id="sec-3">HDFS : Hadoop File System</h2>


</section>
<section id="slide-sec-3-1">
<h3 id="sec-3-1">HDFS</h3>
<ul>
<li>Sistema de almacenamiento distribuido.
<ul>
<li><i>Namenode</i> <code>-&gt;</code> Master</li>
<li><i>Datanode</i> <code>-&gt;</code>  Slaves</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-2">
<h3 id="sec-3-2"><i>Schema on Read</i></h3>
<ul>
<li>Es posible cargar datos sin procesar dentro de Hadoop, la estructura se dará en el tiempo de procesamiento.</li>

<li>Es muy diferente a <i>Schema on Write</i> como el usado en los =RDBM=s
<ul>
<li><i>Schema on Write</i> impone un ciclo de análisis y modelado de datos, así como de su transformación, carga y prueba, antes de los datos puedan ser accesados.</li>
<li>Esto quita mucha flexibilidad: Si se tomaron decisiones incorrectas o los requerimientos cambian, es necesario empezar de nuevo <code>:(</code> .</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-3">
<h3 id="sec-3-3">Ventajas</h3>
<ul>
<li>Archivos muy grandes</li>
<li><i>write once, read many times</i>.</li>
<li>Hardware <i>normal</i></li>

</ul>

</section>
<section id="slide-sec-3-4">
<h3 id="sec-3-4">Desventajas</h3>
<ul>
<li>Acceso a los datos de baja latencia.</li>
<li>Muchos archivos pequeños.</li>
<li>Muchas escrituras, modificaciones</li>

</ul>

</section>
<section id="slide-sec-3-5">
<h3 id="sec-3-5">Tamaño del bloque</h3>
<ul>
<li>Cada <i>file system</i> define un tamaño de bloque, el cual es la cantidad mínima de datos que puede escribir o leer.
<ul>
<li>Típicamente son de <code>kb</code>.</li>

</ul></li>
<li>En <code>HDFS</code>, el bloque es de <code>128 Mb</code> por <i>default</i>.
<ul>
<li>Es el concepto fundamental, no el archivo.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-3-6">
<h3 id="sec-3-6"><i>Namenode</i></h3>
<ul>
<li>Gestiona el <i>filesystem</i>
<ul>
<li>Mantiene el árbol del <i>filesystem</i>.</li>
<li>Mantiene los <code>metadatos</code> de todos los archivos y carpetas del árbol.</li>
<li>Esta información se guarda en disco en dos archivos:
<ul>
<li><code>namespace image</code></li>
<li><code>edit log</code></li>

</ul></li>

</ul></li>
<li>Indica a los <i>datanodes</i> realizar tareas de bajo nivel de <code>I/O</code>.</li>
<li><i>Book Keeper</i>
<ul>
<li>División de archivos en bloques (¿Cómo?)</li>
<li>En qué <i>datanode</i> (¿Quién?)</li>
<li>Monitorea.</li>

</ul></li>
<li>Uso intensivo de <code>RAM</code> y de <code>I/O</code>.</li>
<li>Si se <i>cae</i> el <code>HDFS</code> no puede ser usado
<ul>
<li>Hasta la versión <code>1.x</code> el <i>single point of failure</i>, en Hadoop 2 se incorporó la característica de <i>HIgh Availability</i>.</li>
<li>Su caída puede causar la pérdida total de los datos.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-7">
<h3 id="sec-3-7"><i>Namenode</i></h3>
<ul>
<li>Hadoop proveé de dos formas de aliviar esta situación:
<ul>
<li>Respaldos: Se puede configurar al <i>namenode</i> para que escriba su estado a varios <i>filesystems</i>.</li>
<li><i>Secondary Namenode</i></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-8">
<h3 id="sec-3-8"><i>Namenode</i></h3>

<div class="figure">
<p><img src="./imagenes/Selección_004.png" alt="Selección_004.png" />
</p>
</div>


</section>
<section id="slide-sec-3-9">
<h3 id="sec-3-9"><i>Datanode</i></h3>
<ul>
<li>Lee y escribe los <code>HDFS</code> <i>blocks</i> y los convierte en archivos del <b>FS</b> local.</li>
<li>Se comunica con otros <i>datanodes</i> para la replicación de los datos.</li>
<li>Pueden realizar <i>caching</i> de bloques.</li>

</ul>

</section>
<section id="slide-sec-3-10">
<h3 id="sec-3-10"><i>Datanode</i></h3>

<div class="figure">
<p><img src="./imagenes/Selección_005.png" alt="Selección_005.png" />
</p>
</div>

</section>
<section id="slide-sec-3-11">
<h3 id="sec-3-11"><i>Secondary Name Node</i></h3>
<ul>
<li>Como el <i>namenode</i> sólo hay uno por <i>cluster</i>.</li>
<li>No es un <i>namenode</i>.</li>
<li>Evita que el <code>edit log</code> crezca mucho.</li>
<li>No recibe ni guarda cambios en tiempo real del <code>HDFS</code>.
<ul>
<li>Va atrás del <i>namenode</i>.</li>

</ul></li>
<li>Sólo toma <i>snapshots</i> de la metadata.</li>

</ul>


</section>
<section id="slide-sec-3-12">
<h3 id="sec-3-12">Línea de comandos</h3>
<ul>
<li>Hay muchas maneras de conectarse y usar el <code>HDFS</code>. La línea de comandos es una de ellas.
<ul>
<li>Y espero que ya sepan que es de las más útiles y eficientes.</li>

</ul></li>

<li>Ayuda: <code>hadoop fs -help</code></li>

</ul>

</section>
<section id="slide-sec-3-13">
<h3 id="sec-3-13">Línea de comandos</h3>
<pre><code data-trim>

hadoop fs -cmd <args>
hadoop fs -ls
hadoop fs -mkdir
hadoop fs -copyFromLocal
hadoop fs -copyToLocal
hadoop fs -put archivo archivo_hdfs
hadoop fs -get archivo_hdfs
hadoop fs -cat archivo_hdfs
hadoop fs -cat archivo_hdfs head
hadoop fs -tail archivo_hdfs
hadoop fs -rm archivo_hdfs
</code></pre>



</section>
</section>
<section>
<section id="slide-sec-4" data-background="#000fff">
<h2 id="sec-4">Arquitectura: Ingesta de datos</h2>

</section>
<section id="slide-sec-4-1">
<h3 id="sec-4-1">Decisiones Arquitectónicas</h3>
<ul>
<li>El hecho de que el <code>HDFS</code> permita <i>Schema on Read</i>, no elimina la necesidad de tomar decisiones arquitectónicas en la ingesta de los datos, entre ellas:

<ul>
<li>¿Cómo se guardarán los datos?
<ul>
<li>Capa de almacenamiento</li>
<li>Formatos de archivos</li>
<li>Formatos de compresión</li>

</ul></li>

<li>¿Diseño de esquema de datos?
<ul>
<li>Directorios donde guardar los datos y donde ponerlos luego del procesamiento y analítica.</li>
<li>También en <code>HBase</code> y en <code>Hive</code> se definen esquemas.</li>

</ul></li>
<li>¿Cómo se gestionarán los metadatos?</li>
<li>¿Cómo se administrará la seguridad?
<ul>
<li>Autenticación, cifrado, acceso controlado.</li>

</ul></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-2">
<h3 id="sec-4-2">Capa de almacenamiento: <code>HDFS</code> vs <code>HBase</code></h3>
<ul>
<li><code>HDFS</code>
<ul>
<li>Almacena los datos como archivos</li>
<li><i>Scans</i> rápidos.</li>
<li>Malo para acceso aleatorio en escritura y lectura.</li>

</ul></li>

<li><code>HBase</code>
<ul>
<li>Guarda los datos como archivos de HBase en el <code>HDFS</code>.</li>
<li><i>Scans</i> lentos.</li>
<li>Rápido acceso aleatorio a lectura y escritura.</li>

</ul></li>

<li>En esta clase nos enfocaremos a <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-4-3">
<h3 id="sec-4-3">Formatos de archivos</h3>
<ul>
<li>Tipos de archivos de Hadoop
<ul>
<li>Basados en archivos: <code>SequenceFiles</code>.</li>
<li>Formatos serializados: <code>Avro</code>, <code>Thrift</code>.</li>
<li>Formatos columnares: <code>RCFile</code>, <code>ORCFile</code>, <code>Parquet</code>.</li>

</ul></li>

<li>Debido a que la mayoría de formatos de archivos sólo se puede acceder desde <code>Java</code>, nos enfocaremos en sólo dos: <code>Avro</code> y <code>Parquet</code></li>

</ul>

</section>
<section id="slide-sec-4-4">
<h3 id="sec-4-4">Formatos de archivos</h3>
<ul>
<li><code>Avro</code>
<ul>
<li>Independiente del lenguaje.</li>
<li>Almacena el esquema en el encabezado de cada archivo.</li>
<li>Son comprensibles y divisibles.
<ul>
<li>Soporta compresión con <code>snappy</code>.</li>

</ul></li>
<li>Es recomendable usarlo en la ingesta de datos.</li>
<li>Las fallas sólo afectan a una porción del archivo.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-5">
<h3 id="sec-4-5">Formatos de archivos</h3>
<ul>
<li><code>Parquet</code>
<ul>
<li>Diseñado para proveer procesamiento eficiente a través de varios compoentes de hadoop.</li>
<li>Almacena los datos de manera columnar.</li>
<li>Provee excelentes capacidades de compresión.</li>
<li>Soporta estructuras de datos complejas y anidadas.</li>
<li>Los metadatos están guardados al final del archivo.</li>
<li>Puede escribirse y leerse con las APIs de Avro y con esquemas de Avro.</li>
<li>No son tan buenos para recuperarse de errores.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-6">
<h3 id="sec-4-6">Formatos de compresión</h3>
<ul>
<li>Ayuda a reducir los requerimientos de almacenamiento</li>
<li>Mejora el procesamiento de los datos
<ul>
<li>Disminuye ,a cantidad de I/O en disco y red.</li>

</ul></li>
<li>Para aprovechar las capacidades de procesamiento en paralelo de Hadoop es preferible que el formato sea divisible.</li>

</ul>


</section>
<section id="slide-sec-4-7">
<h3 id="sec-4-7">Formatos de compresión</h3>
<ul>
<li><code>bzip2</code>
<ul>
<li>Excelente factor  de compresión</li>
<li>Pero muuuuuy lento en compresión/decompresión</li>
<li>Divisible</li>

</ul></li>

<li><code>snappy</code>
<ul>
<li>Proyecto de Google.</li>
<li>No es divisible, pero muy eficiente en compresión/decompresión.</li>
<li>Se debe de usar con un formato de archivo que provea la capacidad de contenedor (<code>Avro</code>, <code>SequenceFiles</code>).</li>

</ul></li>

<li><code>gzip</code>
<ul>
<li>No es divisible</li>
<li>Buen factor de comrpesión: 2.5x lo de <code>snappy</code>.</li>
<li>Se debe de usar con un formato de archivo que provea la capacidad de contenedor (<code>Avro</code>, <code>SequenceFiles</code>).</li>

</ul></li>

<li><code>lzop</code>
<ul>
<li>Parecido a <code>snappy</code> en eficiencia de compresión/decompresión.</li>
<li>Divisible, pero requiere una etapa de indexado.</li>
<li>Buena elección para guardar archivos de texto planos que no se pondrán dentro de un contenedor.</li>
<li>Licenciamiento raro (No viene incluido con Hadoop).</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-8">
<h3 id="sec-4-8">Esquema</h3>
<ul>
<li><b>Nota</b>: <i>Basado en <a href="http://shop.oreilly.com/product/0636920033196.do">Hadoop Application Architectures.</a></i></li>

<li>¿Por qué?
<ul>
<li>Estructura de archivos estándar facilita la colaboración entre equipos.</li>
<li>Permite la reutilización de código para procesarla.</li>
<li>Permite reforzar las políticas de acceso y evitar así corrupción de los datos.</li>
<li>Permite identificar que datos han sido procesados completamente y cuales no</li>
<li>Muy parecido a los <code>schemas</code> de PostgreSQL.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-9">
<h3 id="sec-4-9">Esquema Propuesto</h3>
<ul>
<li><code>/user/&lt;username&gt;</code>
<ul>
<li>Datos para experimentar (i.e. no son parte del proceso de negocio).</li>
<li><code>JARs</code>, archivos de configuración.</li>
<li>Sólo debe de tener permisos de R/W el usuario en cuestión.</li>

</ul></li>

<li><code>/etl</code>
<ul>
<li>Datos en sus varias etapas de transformación por el ETL.</li>
<li>Subdirectorios reflejan el <i>workflow</i> de los datos.
<ul>
<li>Los ETL son creados por <b>grupos</b> para <b>aplicaciones</b>.</li>
<li>Además cada subdirectorio tendrá a su vez directorios para cada etapa del proceso:
<ul>
<li><code>input</code> para el lugar donde llegan los archivos</li>
<li><code>procesando</code> para los pasos intermedios (puede haber varios)</li>
<li><code>output</code> para el resultado final</li>
<li><code>rechazados</code> para los registros o archivos que no pudieron ser procesados y que deben de verificarse manualmente.</li>

</ul></li>

</ul></li>
<li>La estructura quedaría así:
<ul>
<li><code>/etl/&lt;grupo&gt;/&lt;aplicación&gt;/&lt;proceso&gt;/{input, procesando, output, rechazados}</code></li>

</ul></li>
<li>Sólo el usuario <code>etl</code> y los usuarios del grupo <code>etl</code> pueden R/W.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-10">
<h3 id="sec-4-10">Esquema Propuesto</h3>
<ul>
<li><code>/tmp</code>
<ul>
<li>Datos temporales generados por usuarios o partes de Hadoop.</li>
<li>Se borra su interior regularmente.</li>
<li>Todos tienen permisos de RW en este directorio.</li>

</ul></li>

<li><code>/data</code>
<ul>
<li>Datos procesados y usados por la organización</li>
<li>Existen controles sobre quién puede o no usar los datos</li>
<li>Los usuarios sólo tienen permisos de lectura.</li>
<li>Los procesos automatizados (y auditados) tienen permisos de escritura.</li>

</ul></li>

<li><code>/app</code>
<ul>
<li>Todo lo requerido por la aplicación de Hadoop para funcionar (salvo datos)</li>
<li>Archivos de Oozie (definiciones de <i>workflows</i>),</li>
<li>Archivos de <code>hql</code>, <code>pig</code>, <code>JARs</code>, <code>UDFs</code>, etc.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-11">
<h3 id="sec-4-11">Otras consideraciones</h3>
<ul>
<li><b>Particionado</b>
<ul>
<li>Ayuda a reducir la cantidad de I/O para procesar los datos.</li>
<li>Es una especie de <i>indexado</i> básico.</li>

</ul></li>

</ul>
<pre class="example">
&lt;nombre del dataset&gt;/&lt;columna sobre la cual particionar&gt;=&lt;valor de la columna&gt;/{archivos}
</pre>

<ul>
<li><b>Denormalizar</b>
<ul>
<li>Ahorras <code>Joins</code> (que son lentos)</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-12">
<h3 id="sec-4-12">Ejercicio I</h3>
<ul>
<li>En este ejercicio prepararemos el <b>esquema</b> de nuestra aplicación de gran escala.</li>
<li>Inicializa el contenedor <code>hadoop-pseudo</code>.</li>
<li>Cambia al usuario <code>itam</code>.</li>
<li>Revisa la estructura de directorios con el usuario <code>hdfs</code>.
<ul>
<li>Esto lo puedes hacer con <code>sudo -u hdfs ...</code></li>

</ul></li>
<li>Crea el esquema de directorios propuesta.
<ul>
<li>Esto lo puedes hacer con <code>sudo -u hdfs ...</code></li>
<li><code>/user/&lt;username&gt;</code>, <code>/etl</code> (para la aplicación <code>ufo</code> y <code>gdelt</code>, el grupo es <code>ds</code>), <code>/tmp</code>, <code>/app</code> y <code>/data</code>.</li>
<li>Las últimas tres están vacías.</li>

</ul></li>
<li>Asigna los permisos adecuados.
<ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>, <a href="http://stackoverflow.com/questions/23095244/add-new-group-to-hdfs">Pregunta de Stackoverflow</a>, <a href="http://spryinc.com/blog/hdfs-permissions-overcoming-permission-denied-accesscontrolexception">Problema del supergroup</a></li>

</ul></li>
<li>Dentro de tu carpeta (siendo el usuario <code>itam</code>), crea la carpeta <code>datasets</code> y adentro <code>ufos</code> y la carpeta <code>gdelt</code>.</li>

</ul>

</section>
<section id="slide-sec-4-13">
<h3 id="sec-4-13">Ejercicio I</h3>
<ul>
<li>Dentro de tu carpeta (siendo el usuario <code>itam</code>), crea la carpeta <code>experimentos</code>.</li>
<li>Carga dos archivos de cada dataset a esta carpeta desde <code>/home/itam/data/</code> usando la línea de comandos.
<ul>
<li>Observa que una de las carpetas es local&#x2026;</li>

</ul></li>

</ul>


<ul>
<li>Verifiquemos que los datos estén bien:</li>

</ul>

<pre><code data-trim>
hadoop fs -cat experimentos/UFO-Dic-2014.tsv | wc -l
hadoop fs -cat experimentos/UFO-Dic-2014.tsv | head
</code></pre>

<ul>
<li>Observa como los datos están en formato de texto, justo como la copia que está en tu disco duro.</li>

</ul>


</section>
<section id="slide-sec-4-14">
<h3 id="sec-4-14">Ejercicio II</h3>
<ul>
<li>En este ejercicio usaremos <code>kite</code>.</li>
<li><a href="http://kitesdk.org/"><code>Kite</code></a> es una herramienta que nos permite cargar y administrar los metadatos de los archivos a Hadoop.
<ul>
<li>Pueden obtener ayuda con <code>kite-dataset help comando</code>.</li>

</ul></li>
<li>Tanto <code>Avro</code>, como <code>Hive Metastore</code> pueden servir para gestionar los metadatos y <code>kite</code> puede trabajar con ambos.</li>
<li>En este ejercicio, nos enfocaremos en el dataset de <code>ufos</code>.</li>
<li>Y a partir de aquí, todos los ejercicios son con el usuario <code>itam</code>.</li>

</ul>

</section>
<section id="slide-sec-4-15">
<h3 id="sec-4-15">Ejercicio II</h3>
<ul>
<li><code>HDFS</code> y <code>Avro</code> para guardar los metadatos.</li>
<li>Infiere el esquema a partir de uno de los archivos:</li>

</ul>

<pre><code data-trim>
kite-dataset csv-schema data/UFO-Nov-2014.tsv --class UFO -o ufos.avsc  --delimiter "\t"
</code></pre>

<ul>
<li>Esto va a marcar un error, arréglalo con <code>sed</code>.
<ul>
<li>Cuando hay <code>/</code> de por medio puedes cambiar el separador de <code>sed</code> por cualquier caracter, ejemplo:</li>

</ul></li>

</ul>

<pre><code data-trim>
sed -e -i 's@cambiar_algo@por_esto@g' archivo
</code></pre>


<ul>
<li>Abre el archivo <code>ufos.avsc</code>, es el esquema en formato <code>avro</code>.</li>
<li>Ahora crearemos el <code>dataset</code> en el <code>hdfs</code>.</li>

</ul>

<pre><code data-trim>
kite-dataset create dataset:hdfs:/user/itam/datasets/ufos --schema ufos.avsc
</code></pre>

<ul>
<li>Observa los cambios ocurridos en la carpeta <code>ufos</code> del <code>hdfs</code>.
<ul>
<li>Recuerda que puedes ver el contenido con el comando <code>hadoop fs -cat</code></li>

</ul></li>

<li>Para verificar que se realizó bien puedes ejecutar:</li>

</ul>

<pre><code data-trim>
kite-dataset schema dataset:hdfs:/user/itam/datasets/ufos
</code></pre>

</section>
<section id="slide-sec-4-16">
<h3 id="sec-4-16">Ejercicio II</h3>
<ul>
<li>Por último, importemos los datos</li>

</ul>

<pre><code data-trim>
kite-dataset csv-import data/UFO-Nov-2014.tsv dataset:hdfs:/user/itam/datasets/ufos --delimiter "\t"
kite-dataset csv-import data/UFO-Dic-2014.tsv dataset:hdfs:/user/itam/datasets/ufos --delimiter "\t"
</code></pre>


<ul>
<li>Veamos que si se copiaron:</li>

</ul>
<pre><code data-trim>
kite-dataset show dataset:hdfs:/user/itam/datasets/ufos
</code></pre>

<ul>
<li>Ahora observa como se ve un conjunto de datos en fornato <code>avro</code>, usando las herramientas de línea de comandos.
<ul>
<li>No lo abras con <code>hadoop fs -cat ...</code> o la consola se dañará&#x2026;</li>

</ul></li>

</ul>


<ul>
<li><b>NOTA</b>: Si algo salió mal, puedes borrar el dataset con</li>

</ul>
<pre><code data-trim>
kite-dataset delete dataset:hdfs:/user/itam/datasets/ufos
</code></pre>

</section>
<section id="slide-sec-4-17">
<h3 id="sec-4-17">Ejercicio II</h3>
<ul>
<li>Ahora guardaremos los datos en  <code>hive metastore</code>.
<ul>
<li>No te preocupes más adelante explicaré que es esto, por el momento piensa en una base de datos para los metadatos.</li>

</ul></li>

<li>Los pasos son casi los mismos que el ejercicio anterior, sólo cambia el destino: ya no es el <code>HDFS</code>, ahora es <code>hive metastore</code>.</li>

<li>Crea el <code>dataset</code></li>

</ul>

<pre><code data-trim>
kite-dataset create ufos --schema ufos.avsc
</code></pre>

<ul>
<li>Para verificar que se realizó bien puedes ejecutar:</li>

</ul>

<pre><code data-trim>
kite-dataset schema ufos
</code></pre>

<ul>
<li>Y para asegurarnos que no son los mismos datos que antes (los guardados en el <code>hdfs</code>), ejecuta</li>

</ul>

<pre><code data-trim>
kite-dataset show ufos
</code></pre>

</section>
<section id="slide-sec-4-18">
<h3 id="sec-4-18">Ejercicio II</h3>
<ul>
<li>Importemos los datos</li>

</ul>

<pre><code data-trim>
kite-dataset csv-import data/UFO-Nov-2014.tsv ufos --delimiter "\t"
kite-dataset csv-import data/UFO-Dic-2014.tsv ufos --delimiter "\t"
</code></pre>


<ul>
<li>Veamos que si se copiaron:</li>

</ul>
<pre><code data-trim>
kite-dataset show ufos
</code></pre>

<ul>
<li><b>NOTA</b>: Si algo salió mal, puedes borrar el dataset con</li>

</ul>
<pre><code data-trim>
kite-dataset delete ufos
</code></pre>


</section>
<section id="slide-sec-4-19">
<h3 id="sec-4-19">Ejercicio III</h3>
<ul>
<li>En este momento, tienes 3 veces los datos en tres formatos diferentes: A
<ol>
<li>Archivo de texto</li>
<li>Archivo <code>avro</code></li>
<li>Guardado como tabla en <code>hive</code> y sus metadatos en el <code>hive metastore</code>.</li>

</ol></li>

<li>Más adelante veremos en detalle las <i>abstracciones</i> y <i>procesadores</i> que tiene <code>Hadoop</code> para manipular y analizar los datos, pero por el momento los usaremos para ver los datos, sin dar mucha explicación.
<ul>
<li>En lo que sigue, observa el código, todo tendrá más sentido cuando expliquemos apropiadamente estas herramientas.</li>

</ul></li>

<li>En este ejercicio, veremos <code>spark</code>, <code>pig</code>, <code>hive</code> e <code>impala</code>.</li>

</ul>

</section>
<section id="slide-sec-4-20">
<h3 id="sec-4-20">Ejercicio III</h3>
<ul>
<li>Empecemos con el archivo de texto (localizados en <code>hdfs://localhost/user/itam/experimentos/</code>)</li>

<li>Usaremos la consola de <code>python</code> de <code>spark</code></li>

</ul>

<pre><code data-trim>
pyspark
</code></pre>

<ul>
<li>La respuesta, luego de varias líneas de texto debe de ser:</li>

</ul>

<pre><code data-trim>
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/

Using Python version 2.7.8 (default, Oct 20 2014 15:05:19)
SparkContext available as sc, HiveContext available as sqlCtx.
>>>
</code></pre>

<ul>
<li>Observa que hay dos contextos al final: <code>SparkContext</code> y <code>HiveContext</code>, estos contextos permiten interactuar con el cluster de Hadoop.</li>

</ul>

</section>
<section id="slide-sec-4-21">
<h3 id="sec-4-21">Ejercicio III</h3>
<ul>
<li>Carguemos como <b>RDD</b> el archivo (en este caso, estamos cargando líneas de texto, no como archivo <code>tsv</code>)</li>

</ul>

<pre><code data-trim>
ufos_nov = sc.textfile("hdfs://localhost/user/itam/experimentos/UFO-Nov-2014.tsv")
</code></pre>

<ul>
<li>Contemos las líneas que hemos cargado</li>

</ul>

<pre><code data-trim>
ufos_nov.count()
</code></pre>

<ul>
<li>Veamos los primeros cinco renglones</li>

</ul>

<pre><code data-trim>
ufos_nov.take(5)
</code></pre>

<ul>
<li>O sólo el primero</li>

</ul>


<pre><code data-trim>
ufos_nov.first()
</code></pre>


<ul>
<li>Si queremos contar el número de estados</li>

</ul>

<pre><code data-trim>
ufos_nov.map(lambda line: (line.split('\t')[2]))\
.distinct()\
.count()
</code></pre>


</section>
<section id="slide-sec-4-22">
<h3 id="sec-4-22">Ejercicio III</h3>
<ul>
<li>¿Qué pasa si queremos cargar el archivo e identificar las columnas?
<ul>
<li>¡Definimos una función en <code>python</code>!</li>

</ul></li>

</ul>

<pre><code data-trim>
def load_tsv(archivo):
    return csv.reader(StringIO(archivo[1]), delimiter='\t')
</code></pre>

<ul>
<li>Y leemos el archivo</li>

</ul>
<pre><code data-trim>
ufos_nov = sc.textFile("hdfs://localhost/user/itam/experimentos/UFO-Nov-2014.tsv").flatMap(load_tsv)
ufos_nov.take(3)[2]
</code></pre>

<ul>
<li>Más adelante veremos como explotar esta estructura.</li>

<li>Para salir presiona <code>Ctrl+C</code> ó <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-23">
<h3 id="sec-4-23">Ejercicio III</h3>
<ul>
<li><code>Pig</code> es una abstracción sobre MapReduce</li>

<li><code>Pig</code> tiene un archivo de configuración localizado en <code>~/.pigbootup</code></li>

<li>Más adelante requeriremos algunos <code>JARs</code> para ejecutar cosas en <code>Pig</code>, en lugar de usarlos desde el sistema de archivos local, los leeremos desde el <code>hdfs</code>.
<ul>
<li>Crea una carpeta llamada <code>lib</code> en <code>/user/itam</code></li>
<li>Copia a esta carpeta los siguientes archivos:
<ul>
<li><code>/usr/lib/pig/datafu-1.1.0-cdh5.4.0.jar</code></li>
<li><code>/usr/lib/pig/piggybank.jar</code></li>
<li><code>/usr/lib/pig/lib/avro-1.7.6-cdh5.4.0.jar</code></li>
<li><code>/usr/lib/pig/lib/snappy-java-1.0.5.jar</code></li>
<li><code>/usr/lib/pig/lib/json-simple-1.1.jar</code></li>

</ul></li>

</ul></li>

<li>Crea el archivo <code>.pigbootup</code> en tu carpeta <code>$HOME</code> (i.e. <code>/home/itam</code>)</li>

<li>Agrega lo siguiente:</li>

</ul>
<pre><code data-trim>
REGISTER hdfs://localhost/user/itam/lib/datafu-1.1.0-cdh5.4.0.jar
REGISTER hdfs://localhost/user/itam/lib/piggybank.jar
REGISTER hdfs://localhost/user/itam/lib/avro-1.7.6-cdh5.4.0.jar
REGISTER hdfs://localhost/user/itam/lib/snappy-java-1.0.5.jar
REGISTER hdfs://localhost/user/itam/lib/json-simple-1.1.jar
</code></pre>



<ul>
<li>Para ejecutarlo</li>

</ul>

<pre><code data-trim>
pig -useHCatalog
</code></pre>


</section>
<section id="slide-sec-4-24">
<h3 id="sec-4-24">Ejercicio III</h3>
<ul>
<li>Para replicar lo que hicimos con <code>Spark</code>:</li>

</ul>

<pre><code data-trim>
ufos_dic = LOAD 'experimentos/UFO-Dic-2014.tsv' using PigStorage('\t')  \
           AS (Timestamp:chararray, \
               City:chararray, State:chararray, \
               Shape:chararray, Duration:chararray, \
               Summary:chararray, Posted:chararray);
DESCRIBE ufos_dic;
head = LIMIT ufos_dic 5;
DUMP head;
</code></pre>

<ul>
<li>Puedes seguir la ejecución vía web  <a href="http://0.0.0.0:8088">aquí</a>.</li>

<li>Nota el uso de mayúsculas para las palabras clave de <code>Pig</code>.</li>

</ul>

</section>
<section id="slide-sec-4-25">
<h3 id="sec-4-25">Ejercicio III</h3>
<ul>
<li>Ahora usemos los archivos con formato <code>avro</code> y observemos como, dado que tienen metadatos, es mucho más fácil.
<ul>
<li>Nota lo limpio que va a quedar el código ahora&#x2026;</li>

</ul></li>

</ul>

<pre><code data-trim>
ufos = LOAD 'datasets/ufos' USING org.apache.pig.piggybank.storage.avro.AvroStorage();
DESCRIBE ufos;
ILLUSTRATE ufos;
head = LIMIT ufos 5;
DUMP head;
</code></pre>

<ul>
<li>Observa como no hubo problemas con el header del archivo!
<ul>
<li>¡En el ejercicio anterior (tanto con <code>pig</code> como con <code>spark</code>) era la primera línea!</li>

</ul></li>

<li>Para ver los diferentes estados</li>

</ul>
<pre><code data-trim>
states = DISTINCT (FOREACH ufos GENERATE State);
DUMP states;
</code></pre>

<ul>
<li>Para salir presiona <code>Ctrl+C</code> ó <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-26">
<h3 id="sec-4-26">Ejercicio III</h3>
<ul>
<li>Por último usaremos las herramientas parecidas a <code>SQL</code> que proveé Hadoop: <code>Hive</code> e <code>Impala</code>.</li>

<li>Usaremos el <code>Hive Metastore</code>.
<ul>
<li>Aunque podríamos usar el <code>hdfs</code> o <code>avro</code> en el <code>hdfs</code>.</li>

</ul></li>

<li>Para ejecutar el cliente de <code>Hive</code></li>

</ul>

<pre><code data-trim>
beeline -u jdbc:hive2://localhost:10000
</code></pre>

<ul>
<li>Veámos que tablas hay disponibles</li>

</ul>

<pre><code data-trim>
show tables;
</code></pre>

<ul>
<li>Obtengamos los primeros 5</li>

</ul>

<pre><code data-trim>
select * from ufos limit 5;
</code></pre>

<ul>
<li>Contar los estados diferentes:</li>

</ul>

<pre><code data-trim>
select count(distinct State) from ufos;
</code></pre>

<ul>
<li>Ver el plan de ejecución del <i>query</i></li>

</ul>

<pre><code data-trim>
explain select count(distinct State) from ufos;
</code></pre>


<ul>
<li>Compara con este <i>query</i>
<ul>
<li>¿Cuál es la diferencia?</li>

</ul></li>

</ul>

<pre><code data-trim>
explain select count(*) from (select distinct State from ufos) as t;
</code></pre>

<ul>
<li>Para salir presiona <code>Ctrl+C</code> ó <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-27">
<h3 id="sec-4-27">Ejercicio III</h3>
<ul>
<li>Para iniciar <code>Impala</code></li>

</ul>

<pre><code data-trim>
impala-shell
</code></pre>

<ul>
<li>Debido a que Impala <b>no</b> es una abstracción de <b>MapReduce</b>, sus tiempos son impresionantemente rápidos</li>

</ul>

<pre><code data-trim>
invalidate metadata; # Siempre ejecutarlo cuando se modifiquen las tablas fuera de Impala
show tables;
describe ufos;
select * from ufos limit 5; # Este quizá tarde un poco... (warming up)
select * from ufos limit 15; # Debería de volar
</code></pre>

<ul>
<li>Top 5 de avistamientos por estado</li>

</ul>

<pre><code data-trim>
select state, count(*) as conteo from ufos group by state order by conteo desc limit 5;
</code></pre>

<ul>
<li>Para salir presiona <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-28">
<h3 id="sec-4-28">Ejercicio III: Recapitulando</h3>
<ul>
<li>Vimos diferentes maneras de interactuar con los datos
<ul>
<li>Lo vamos a profundizar luego.</li>

</ul></li>

<li>Es importante notar que aunque usamos diferentes herramientas para cada tipo de archivo (Texto, Avro, Tabla),  <i>todas</i> las herramientas pueden ver <i>todos</i> los formatos.
<ul>
<li>Casi&#x2026;por lo menos los mostrados aquí.</li>
<li>Por ejemplo, podemos usar <code>pig</code> para leer las tablas de <code>hive</code>, cambiando el <code>LOAD</code> como sigue:</li>

</ul></li>

</ul>
<pre><code data-trim>
ufos = load 'ufos' using org.apache.hive.hcatalog.pig.HCatLoader();
describe ufos;
illustrate ufos;
...
</code></pre>

<ul>
<li>Es importante notar también, que cada herramienta es para un diferente proceso (ingeniería, analítica, etc.)</li>

<li>Estamos explorando los datos, aún no establecemos un <i>workflow</i>
<ul>
<li>También lo veremos más adelante.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-29">
<h3 id="sec-4-29">Ejercicio IV</h3>
<ul>
<li>Repite todo lo anterior para cargar dos archivos de <code>gdelt</code> de tu elección.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-5" data-background="#000fff">
<h2 id="sec-5">YARN</h2>

</section>
<section id="slide-sec-5-1">
<h3 id="sec-5-1">YARN</h3>
<ul>
<li>La infraestructura de Hadoop <code>0.x</code> y <code>1.x</code> era monolítica, por eso fue rediseñada.</li>
<li><code>YARN</code>: <i>Yet Another Resource Negotiator</i>.</li>
<li>La gestión de recursos es extraída de los paquetes de <code>MapReduce</code> para que puedan ser utilizadas por otros componentes.</li>
<li>Aportaciones
<ul>
<li>Escalabilidad.</li>
<li>Compatibilidad con <code>MapReduce</code>.</li>
<li>Mejoras en la gestión del <i>cluster</i>.</li>
<li>Soporte para otros modelos de programación (además de <code>MapReduce</code>).
<ul>
<li><i>Graph processing</i></li>
<li><i>Message Passing Interface</i> (<b>MPI</b>).</li>
<li>Soporte para procesamiento <i>real-time</i> o <i>near real-time</i>.
<ul>
<li><code>MapReduce</code> es <i>batch-oriented</i>.</li>

</ul></li>

</ul></li>
<li>Agilidad.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-5-2">
<h3 id="sec-5-2">YARN</h3>
<ul>
<li>Se dividieron las dos responsabilidades del <i>JobTracker</i>:
<ul>
<li>Gestión de recursos (<i>Resource Management</i>)</li>
<li>Asignación y vigilancia de trabajos (<i>Job scheduling-monitoring</i>)</li>

</ul></li>

<li>La idea es tener un <i>ResourceManager</i> global y un <i>NodeManager</i> por
nodo esclavo, los cuales forman un sistema para la administración de
aplicaciones distribuidas.</li>

<li>El <i>ResourceManager</i> tiene dos componentes principales:
<ul>
<li><i>Scheduler</i>: Asigna los recursos para las aplicaciones (<i>pluggeable</i>).</li>
<li><i>Application Manager</i>: Responsable de aceptar las solicitudes de
trabajos, negociando al principio para ejecutar el <i>Application
Master</i> específico y provee un servicio de reinicio, por si el
<i>Application Master</i> falla.</li>

</ul></li>

<li>En cada nodo:

<ul>
<li>El <i>Application Master</i>: Negocia sus recursos con el <i>Scheduler</i>,</li>

</ul>
<p>
monitorea sus avances y reporta su estatus.
</p>

<ul>
<li>El <i>NodeManager</i> es el responsable de los contenedores,
monitorear el uso de recursos y reportar todo al
<i>ResourceManager</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-5-3">
<h3 id="sec-5-3">Arquitectura MapReduce Hadoop 1.x</h3>

<div class="figure">
<p><img src="./imagenes/MRArch.png" alt="MRArch.png" />
</p>
</div>

</section>
<section id="slide-sec-5-4">
<h3 id="sec-5-4">Arquitectura Hadoop 2.x</h3>

<div class="figure">
<p><img src="./imagenes/Selección_003.png" alt="Selección_003.png" />
</p>
</div>


</section>
<section id="slide-sec-5-5">
<h3 id="sec-5-5">Cambios 1.x -&gt; 2.x</h3>

<div class="figure">
<p><img src="./imagenes/yarn.png" alt="yarn.png" />
</p>
</div>


</section>
<section id="slide-sec-5-6">
<h3 id="sec-5-6">Multiparadigma en Hadoop 2.x</h3>

<div class="figure">
<p><img src="./imagenes/YARN.png" alt="YARN.png" />
</p>
</div>


</section>
</section>
<section>
<section id="slide-sec-6" data-background="#000fff">
<h2 id="sec-6">Procesamiento</h2>

</section>
<section id="slide-sec-6-1">
<h3 id="sec-6-1">Tipos</h3>
<ul>
<li>MapReduce</li>
<li>Spark</li>
<li>Impala</li>

</ul>

</section>
<section id="slide-sec-6-2">
<h3 id="sec-6-2">MapReduce en Hadoop</h3>
<ul>
<li>Principal <i>framework</i> de ejecución de <code>Apache Hadoop</code>.</li>
<li>Inspirado en las operaciones <b>MAP</b> y <b>REDUCE</b> de los lenguajes funcionales.</li>
<li>Modelo de programación para proceso de datos distribuido  y paralelo.</li>
<li>Divide las tareas (<i>jobs</i>) en fases de <i>mapeo</i> y fases de <i>reducción</i>.</li>
<li>Los desarrolladores crean tareas <i>MapReduce</i> para Hadoop usando datos guardados en el <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-6-3">
<h3 id="sec-6-3">MapReduce: Ventajas</h3>
<ul>
<li><i>Fault-tolerant</i>.</li>
<li>Esconde los detalles de implementación a los programadores.</li>
<li>Escala con el tamaño de los datos.</li>

</ul>


</section>
<section id="slide-sec-6-4">
<h3 id="sec-6-4">MapReduce</h3>
<ul>
<li>Dos fases de procesamiento:
<ul>
<li><i>key-value</i> como Input y Output</li>
<li>El programador especifica:
<ul>
<li>Tipos de <i>key-value</i></li>
<li>Funciones: <code>MAP</code> y <code>REDUCE</code>.</li>

</ul></li>

</ul></li>

</ul>


</section>
<section id="slide-sec-6-5">
<h3 id="sec-6-5">Una pequeña regresión&#x2026;</h3>

</section>
<section id="slide-sec-6-6">
<h3 id="sec-6-6">map-reduce: Matemáticamente</h3>
<PRE><CODE DATA-TRIM>
map: (k1, v1) -> list(k2, v2)
</CODE></PRE>

<ul>
<li><code>map</code> Mapea (aplica una función <i>f</i>) un conjunto de entrada de pares <i>key-value</i> a otro conjunto intermedio de <i>key-values</i></li>

</ul>


</section>
<section id="slide-sec-6-7">
<h3 id="sec-6-7">map-reduce: Matemáticamente</h3>
<PRE><CODE DATA-TRIM>
reduce: (k2, list(v2)) -> list(k3, v3)
</CODE></PRE>

<ul>
<li><code>reduce</code>  Aplica una función <i>g</i> a todos los valores (<i>values</i>) asociados a una llave (<i>key</i>) y acumula el resultado. Emite pares de <i>key-values</i>.</li>

</ul>

</section>
<section id="slide-sec-6-8">
<h3 id="sec-6-8">Python <code>map</code></h3>
<div class="org-src-container">

<pre  class="src src-python"># Equivalente en for-loop

items = [1,2,3,4,5]
cuadrados = []
for x in items:
    cuadrados.append(x**2)

print cuadrados
</pre>
</div>


<div class="org-src-container">

<pre  class="src src-python"># Usando la funcion map(function, sequence)

items = [1,2,3,4,5]

print list(map((lambda x: x**2), items))
</pre>
</div>


</section>
<section id="slide-sec-6-9">
<h3 id="sec-6-9">Python <code>reduce</code></h3>
<div class="org-src-container">

<pre  class="src src-python"># Equivalente en for-loop
L = [1,2,3,4]
result = L[0]
for x in L[1:]:
    result = result*x

print result
</pre>
</div>

<div class="org-src-container">

<pre  class="src src-python"># Usando la funcion reduce(funcion, secuencia)
print reduce((lambda x,y: x*y), [1,2,3,4])
</pre>
</div>

</section>
<section id="slide-sec-6-10">
<h3 id="sec-6-10">Python <code>map</code> y <code>reduce</code></h3>
<div class="org-src-container">

<pre  class="src src-python">a = range(1, 4)
b = range(4, 9)
c = range(9, 15)
print "a -&gt;  %s, b -&gt; %s , c -&gt; %s" % (a, b, c)

L1 = map(lambda x:len(x), [a,b,c])
print "L1 -&gt; %s" % L1

L2 = reduce(lambda x, y: x+y, L1)
print "L2 -&gt; %s" % L2
</pre>
</div>



</section>
<section id="slide-sec-6-11">
<h3 id="sec-6-11">MapReduce y map-reduce</h3>
<ul>
<li>Básicamente es lo mismo, pero&#x2026;</li>
<li><code>map</code>, <code>reduce</code> (entre otras) son parte de lenguajes funcionales.</li>
<li><code>MapReduce</code> es la aplicación de esta idea aplicada a problemas <i>vergonzosamente</i> <i>paralelos</i>.
<ul>
<li>Ver la carpeta <code>docs</code> para el artículo de <b>Google</b> sobre <code>MapReduce</code>.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-6-12">
<h3 id="sec-6-12">GNU Parallel</h3>
<div class="org-src-container">

<pre  class="src src-sh">find ./data/books -type f | parallel -j0 egrep -i  '\[\[:digit:\]\]' {} | awk '{s+=$1} END {print s}'
</pre>
</div>


<ul>
<li><b>¿Puedes identificar las partes <code>map</code> y <code>reduce</code>?</b></li>
<li>Esto ya es un <code>MapReduce</code>.</li>

</ul>


</section>
<section id="slide-sec-6-13">
<h3 id="sec-6-13">MapReduce en Hadoop</h3>
<ul>
<li>A nivel programático:
<ul>
<li><i>Data</i> de entrada</li>
<li>Programa MapReduce</li>
<li>Configuración</li>
<li>Subtareas: <code>map</code> y <code>reduce</code></li>

</ul></li>

</ul>


</section>
<section id="slide-sec-6-14">
<h3 id="sec-6-14">MapReduce: <i>Mapper</i></h3>
<ul>
<li>Hadoop divide la entrade de datos al <i>job</i> MapReduce en pedazos de tamaño fijo llamados <i>input splits</i>.</li>
<li>Hadoop crea una tarea <code>map</code> para cada <i>input split</i>.</li>
<li><code>map</code> escribe al <i>file system</i> local.
<ul>
<li>Si el <code>reducer</code> tiene éxito se borra la salida del <i>mapper</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-6-15">
<h3 id="sec-6-15">Map only</h3>

<div class="figure">
<p><img src="./imagenes/map_only.png" alt="map_only.png" />
</p>
</div>


</section>
<section id="slide-sec-6-16">
<h3 id="sec-6-16">MapReduce: <i>Reducer</i></h3>
<ul>
<li>La entrada es la salida de (posiblemente) todos los <i>mappers</i>.</li>
<li>Estas se transmiten vía red al nodo donde corre el <i>reducer</i>.</li>
<li>La salida se guarda en el <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-6-17">
<h3 id="sec-6-17">Map, One reduce</h3>

<div class="figure">
<p><img src="./imagenes/map_one_reduce.png" alt="map_one_reduce.png" />
</p>
</div>

</section>
<section id="slide-sec-6-18">
<h3 id="sec-6-18">MapReduce</h3>

<div class="figure">
<p><img src="./imagenes/map_reduce.png" alt="map_reduce.png" />
</p>
</div>


</section>
<section id="slide-sec-6-19">
<h3 id="sec-6-19">MapReduce: <i>Combiner</i></h3>
<ul>
<li>Es una medida de optimización.</li>
<li>Es para ahorrar ancho de banda.</li>
<li>Una especie de <i>reducer</i> local.</li>
<li>No es parte (estrictamente) del MapReduce
<ul>
<li>Por eso no lo había mencionado.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-6-20">
<h3 id="sec-6-20">Hello World!: Word count</h3>
<div class="outline-text-3" id="text-6-20">
</div></section>
<section id="slide-sec-6-20-1">
<h4 id="sec-6-20-1">Word count</h4>
<ul>
<li>Es el ejemplo <i>Hola Mundo</i> de Apache Hadoop.</li>
<li>No sólo eso, es el ejemplo que se utiliza en el trabajo seminal
<ul>
<li><b>MapReduce: Simplified Data Processing on Large Clusters</b> <i>(2006)</i>.</li>
<li>En la carpeta <code>docs</code> como ya había dicho.</li>

</ul></li>
<li>Solamente 1 <code>Map</code> y 1 <code>Reduce</code>.</li>

</ul>


</section>
<section id="slide-sec-6-20-2">
<h4 id="sec-6-20-2">Word count</h4>
<ul>
<li><b>mapper</b>
<ul>
<li><code>k1</code> -&gt; nombre de archivo</li>
<li><code>v1</code> -&gt; texto del archivo</li>
<li><code>k2</code> -&gt; palabra</li>
<li><code>v2</code> -&gt; "1"</li>

</ul></li>

<li><b>reducer</b>
<ul>
<li><code>k2</code> -&gt; palabra</li>
<li>list(v2) -&gt; (1,1,1,1,1,1,&#x2026;, 1)</li>

</ul>
<p>
Suma los "1" y produce una lista de
</p>

<ul>
<li>k3 -&gt; palabra</li>
<li>v3 -&gt; suma</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-6-20-3">
<h4 id="sec-6-20-3">Word count</h4>

<div class="figure">
<p><img src="./imagenes/word_count.png" alt="word_count.png" />
</p>
</div>

</section>
<section id="slide-sec-6-20-4">
<h4 id="sec-6-20-4">Pseudocódigo</h4>
<pre><code data-trim>
map (String key, String value)
   for each word w in value
      Emit(w, 1)

reduce (String key, Iterator values)
   int wordcount = 0
   for each v in values
      wordcount += v
      Emit(key, wordcount)

</code></pre>

</section>
<section id="slide-sec-6-20-5">
<h4 id="sec-6-20-5">Mockup</h4>
<ul>
<li>Ver los archivos <code>word_count.py</code> y <code>mapreduce.py</code> en la carpeta <code>mock</code>.</li>

</ul>

<pre><code data-trim>
chmod +x word_count.py
python word_count.py
</code></pre>

<ul>
<li>Este es un ejemplo de mentiritas, no usa Apache Hadoop.</li>

</ul>



</section>
<section id="slide-sec-6-21">
<h3 id="sec-6-21">Ejercicio</h3>
<ul>
<li>Diseñe el <b><b>MapReduce</b></b> para lo siguiente:
<ul>
<li>Encontrar el máximo de un conjunto de datos.</li>
<li>Encontrar el promedio y desviación estándar de unos datos.</li>
<li>Encontrar el top 10 de una cantidad.</li>
<li>Contar por grupo</li>

</ul></li>

</ul>



</section>
<section id="slide-sec-6-22">
<h3 id="sec-6-22">Spark</h3>
<div class="outline-text-3" id="text-6-22">
</div></section>
<section id="slide-sec-6-22-1">
<h4 id="sec-6-22-1">Spark</h4>
<ul>
<li><i>Framework</i> de cómputo general para <i>clusters</i></li>
<li>Ejecuta en <code>YARN</code>
<ul>
<li>Aunque también puede hacer <i>standalone</i>, o ejecutar sobre <code>EC2</code> o <code>Mesos</code>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-6-22-2">
<h4 id="sec-6-22-2">Resilient Distributed Datasets (RDDs)</h4>
<ul>
<li>Es una de las ideas principales de Spark.</li>
<li><code>RDDs</code> es una abstracción que representa una colleción de objetos de sólo lectura que está particionada a lo largo de varias máquinas.</li>
<li>Sus ventajas:
<ul>
<li>Pueden ser reconstruidas a partir de su <i>lineage</i>. (Soportan fallos&#x2026;)</li>
<li>Pueden ser accesadas vía operaciones en paralelo, parecidas a MapReduce.</li>
<li>Son <i>cached</i> en memoria para su uso inmediato.</li>
<li>Fueron construidas para ser almacenadas de manera distribuida.</li>

</ul></li>

</ul>



</section>
</section>
<section>
<section id="slide-sec-7" data-background="#000fff">
<h2 id="sec-7">Abstracciones</h2>

</section>
<section id="slide-sec-7-1">
<h3 id="sec-7-1">Pig</h3>
<div class="outline-text-3" id="text-7-1">
</div></section>
<section id="slide-sec-7-1-1">
<h4 id="sec-7-1-1">Pig</h4>
<ul>
<li>Proyecto de Apache</li>
<li>Abstracción encima de Hadoop
<ul>
<li><i>Pig Latin</i> compila a <code>MapReduce</code></li>
<li>En cierta forma <i>Pig Latin</i> es para analistas, <i>data scientist</i> y estadísticos.</li>
<li><code>MapReduce</code>  es para programadores (aunque los <i>data scientist</i> deberían de poder hacerlo también)</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-1-2">
<h4 id="sec-7-1-2">Pig</h4>
<ul>
<li>Pig es un <i>data flow programming language</i></li>
<li>Es decir,
<ul>
<li>Ejecuta paso a paso</li>
<li>Cada paso es una transformación de datos</li>

</ul></li>
<li>En cambio <code>SQL</code> es un conjunto de <i>constraints</i> que en conjunto definen el resultado buscado.</li>

</ul>

</section>
<section id="slide-sec-7-1-3">
<h4 id="sec-7-1-3">Pig</h4>
<ul>
<li>¿Qué cosas puede hacer?
<ul>
<li><code>joins</code></li>
<li><code>sorts</code></li>
<li><code>filters</code></li>
<li><code>group by</code></li>
<li><i>User defined functions</i> <code>UDF</code>'s</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-1-4">
<h4 id="sec-7-1-4">Pig</h4>
<ul>
<li>¿Qué cosas <b>puedo</b> hacer?

<ul>
<li><code>ETLs</code>
<ul>
<li>Limpiar.</li>
<li><i>Joins</i> gigantes.</li>

</ul></li>

<li>Búsqueda en <i>Raw</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-1-5">
<h4 id="sec-7-1-5">Pig</h4>
<ul>
<li>Componentes
<ul>
<li><i>Pig Latin</i></li>
<li><code>Grunt</code>
<ul>
<li>Local</li>
<li>MapReduce</li>

</ul></li>
<li><code>Pig compiler</code></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-1-6">
<h4 id="sec-7-1-6">Pig</h4>
<ul>
<li>Es posible ejecutar también <i>scripts</i> de <i>Pig Latin</i> (terminación <code>.pig</code>) sin entrar a <code>grunt</code>.</li>

</ul>

<pre><code data-trim>
pig script_file.pig

# Si quieren pasar parámetros
pig -p var=bla/bla var2=bla/bla/bla script_file.pig
</code></pre>

<ul>
<li>Y usarse desde programas en <code>Java</code> con la clase <code>PigServer</code>.
<ul>
<li>Como una especie de <code>JDBC</code>, pero para <i>Pig Latin</i>.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-7-1-7">
<h4 id="sec-7-1-7">Pig: <i>Building blocks</i></h4>
<p>
Fields
</p>
<pre><code data-trim>
'Adolfo'
</code></pre>

<p>
Tuplas
</p>
<pre><code data-trim>
('Adolfo', 3, 8.17, 23)
</code></pre>

<p>
<i>Bags</i>
</p>
<pre><code data-trim>
{('Adolfo', 3, 8.17, 23), ('Paty', 3.14, 9, 'A')}
</code></pre>



</section>
<section id="slide-sec-7-1-8">
<h4 id="sec-7-1-8">Ejercicio</h4>
<pre><code data-trim>
ufos = load 'ufos' using org.apache.pig.piggybank.storage.avro.AvroStorage();
a_imprimir = limit ufos 5;
por_estado = group ufos by State;
describe por_estado;
explain por_estado;
illustrate por_estado;
# itera sobre cada elemento del bag
conteo = foreach por_estado generate group count_star(ufos);
ordenados = order conteo by $1 desc;
top_five = limit ordenado 5;
unicos = distinct conteos;
muestreo = sample por_estado 0.1;
filtrados = filter conteos by substring(group, 0, 2) == 'W';
mayores = filter conteos by $1 > 50;
</code></pre>

</section>
<section id="slide-sec-7-1-9">
<h4 id="sec-7-1-9">Pig: JOINS</h4>
<ul>
<li>Cargamos fuente 1</li>
<li>Cargamos fuente 2</li>
<li>Unimos las fuentes (<i>bags</i>) mediante una llave</li>
<li>Súper simple</li>

</ul>

<p>
Pig soporta <i>inner joins</i> (valor por omisión), <i>left outer joins</i> (y
<i>right</i> también) y <i>full outer</i> joins.
</p>


<pre><code data-trim>
fuentes_unidas = join fuente1 by (keys) [left|right|full outer] fuente2 by (keys);
</code></pre>

<p>
Además <code>Pig</code> soporta <code>cogroup</code> además de los <code>joins</code> (el <code>cogroup</code>
preserva la estructura de las fuentes y crea tuplas por cada llave)
</p>

<pre><code data-trim>
fuentes_unidas = cogroup fuente1 by (keys) fuente2 by (keys);
</code></pre>


</section>
<section id="slide-sec-7-1-10">
<h4 id="sec-7-1-10">Pig: Ejemplo de JOINs y COGROUPs</h4>
<pre><code data-trim>

# Fuentes de datos

mascotas: (dueño, mascotas)
----------------------
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)

amigos: (amigo1, amigo2)
----------------------
(Diana, Adolfo)
(Gabriel, Adolfo)
(Shanti, Paty)


COGROUP mascotas by dueño, amigos por amigo2;
---------------------------------------------
(Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)}, {(Diana, Adolfo), (Gabriel, Adolfo)})
(Paty, {(Paty, perro), (Paty, gato)}, {(Shanti, Paty)})

JOIN mascotas by dueño, amigos por amigo2;
-------------------------------------------
(Adolfo, tortuga, Diana)
(Adolfo, tortuga, Gabriel)
(Adolfo, pez, Diana)
(Adolfo, pez, Gabriel)
(Adolfo, gato, Diana)
(Adolfo, gato, Gabriel)
(Paty, perro, Shanti)
(Paty, gato, Shanti)
</code></pre>

</section>
<section id="slide-sec-7-1-11">
<h4 id="sec-7-1-11">Aclaraciones sobre GROUP y FLATTEN</h4>
<ul>
<li><code>FLATTEN</code> elimina un nivel anidamiento</li>

</ul>

<pre><code data-trim> pig
# Datos:
# (Adolfo, (tortuga, pez, gato))
# (Paty, (perro, gato))
# FLATTEN eliminaría los bags internos
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)
</code></pre>

<ul>
<li><code>GROUP .. BY</code> organiza los <i>bags</i> en <i>bags</i></li>

</ul>
<pre><code data-trim> pig
# Siguiendo con los datos anteriores de mascotas
GROUP mascotas BY dueño;

# ( Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)} )
# ( Paty, {(Paty, perro), (Paty, gato)} )

</code></pre>

<ul>
<li>En cierto sentido <code>FLATTEN</code> y <code>GROUP .. BY</code> son operaciones inversas
entre sí.</li>

</ul>


</section>
<section id="slide-sec-7-2">
<h3 id="sec-7-2">Hive</h3>
<div class="outline-text-3" id="text-7-2">
</div></section>
<section id="slide-sec-7-2-1">
<h4 id="sec-7-2-1">Hive</h4>
<ul>
<li>Proyecto de Apache.</li>
<li><span class="underline">Abstracción</span> pra modelar y procesar datos en Hadoop.</li>
<li>Proveé de una manera de estructurar datos guardados en el <code>HDFS</code>.</li>
<li>Permite crear <span class="underline">queries</span> muy similares a <code>SQL</code> (llamado <code>HQL</code>) y correrlos contra los datos.</li>
<li>Contiene un almacén de metadatos (<code>HCatalog</code>), que además puede ser compartido con otras interfaces como <code>Pig</code>, <code>MapReduce</code>, etc.</li>
<li>Da Acceso al <code>HDFS</code> y <code>HBASE</code>.</li>

</ul>

</section>
<section id="slide-sec-7-2-2">
<h4 id="sec-7-2-2">Bibliografía recomendada</h4>
<ul>
<li>Sitio web de Hive</li>
<li>Hadoop: The Definitive Guide</li>
<li>Programming Hive</li>

</ul>


</section>
<section id="slide-sec-7-2-3">
<h4 id="sec-7-2-3">Arquitectura de Apache Hive</h4>

<div class="figure">
<p><img src="./imagenes/hive-remote.jpeg" alt="hive-remote.jpeg" />
</p>
</div>



</section>
<section id="slide-sec-7-3">
<h3 id="sec-7-3">Impala</h3>
<div class="outline-text-3" id="text-7-3">
</div></section>
<section id="slide-sec-7-3-1">
<h4 id="sec-7-3-1">Impala</h4>


</section>
</section>
<section>
<section id="slide-sec-8" data-state="soothe">
<h2 id="sec-8">Disclaimer</h2>

<p>
Algunas imágenes se tomaron de los libros <i>Professional Hadoop Solutions</i>
de <b>Wrox</b> y de la página de <a href="http://hortonworks.com/hadoop/yarn/">Hortonworks</a>. Las otras son mías.
</p>

<p>
Las tablas de la sección <i>cluster</i> de Hadoop, se tomaron de <a href="http://hortonworks.com/">Hortonworks.</a>
</p>
</section>
</section>
</div>
</div>
<p> Creado por Adolfo De Unánue Tiscareño. </p>

<script src="http://cdn.jsdelivr.net/reveal.js/2.5.0/lib/js/head.min.js"></script>
<script src="http://cdn.jsdelivr.net/reveal.js/2.5.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: true,
rollingLinks: false,
keyboard: true,
overview: true,
width: 1200,
height: 800,
margin: 0.10,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'fade', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
]
});
</script>
</body>
</html>

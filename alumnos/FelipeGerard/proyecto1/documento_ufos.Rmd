---
title: "Proyecto 1. UFOs"
author: "Felipe Gerard"
date: "1 de marzo de 2015"
output: html_document
---
```{r, echo=FALSE}
setwd('/Users/Felipe/big-data/alumnos/FelipeGerard/proyecto1')
library(dplyr, warn.conflicts = F, quietly = T)
```

En este proyecto analizaremos la base de avistamientos de UFOs de la NUFORC (National UFO Reporting Center).

### Bajar la información en máquinas de AWS usando `parallel`

Antes que nada, hacemos un scrapping en R del índice de la base para obtener las URLs de cada parte con `get_urls.R`:

```{r, eval=FALSE}
library(rvest)
setwd('/Users/Felipe/big-data/alumnos/FelipeGerard/proyecto1')

base_url <- 'http://www.nuforc.org/'
index_url <- 'http://www.nuforc.org/webreports/ndxevent.html'
webreports_url <- 'http://www.nuforc.org/webreports/'
index <- html(index_url)

data_urls <- index %>%
  html_nodes(css = 'a') %>%
  html_attr('href') %>%
  grep(pattern = 'ndxe', value=T)
data_urls <- paste0(webreports_url, data_urls)

#write(data_urls, 'data_urls')
```

```{r, }
read.table('data_urls', nrows = 6, col.names = 'url')
```

Luego, como la información está en formato HTML, necesitaremos instalar R y `rvest` en las máquinas de AWS para poderla bajar en paralelo. Instalamos R mediante el archivo `instala_R.sh`:

```{shell}
parallel --nonall --slf instancias_aws "sudo apt-get update; sudo apt-get install -y r-base-core"
```

Y luego `rvest` usando `instala_rvest.sh`:

```{shell}
#! /bin/zsh
parallel --nonall --basefile instala_rvest.R --slf instancias_aws \
"
  sudo apt-get install -y libcurl4-gnutls-dev;
  sudo apt-get install -y r-cran-xml;
  sudo R --no-save < ./instala_rvest.R
"
```

Donde `instala_rvest.R` es lo siguiente:

```{r, eval=FALSE}
install.packages('rvest', repos="http://cran.us.r-project.org", dependencies=TRUE)
```


Ahora sí podemos bajar la información en paralelo con el script `get_data_aws.sh`:
```{shell}
#! /bin/zsh
sed -n 6,10p data_urls \
| sed "s/.*\///" \
| parallel --progress --basefile get_data.R --slf instancias_aws \
   "echo {} \
   | ./get_data.R \
   | awk ' {if(NR==1){print}} !/Date...Time/{print}' \
   | sed -e 's/^\"//' -e 's/\"$//' \
         -e 's/\" \"/|/g' \
         -e 's/Date...Time/Date|Time/' \
         -e 's/\([0-9]\+[^0-9]\+\)//' \
         -e 's/ /|/' \
         -e 's/^\([0-9]\/\)/0\1/' \
         -e 's/\([0-9]\{2\}\/\)\([0-9]\{2\}\/\)\([0-9]\{2\}\)/\2\1\3/' \
         -e 's/^\([0-9]\/\)/0\1/' \
   | tr '|' '\t' \
   > ~/ufo_data/{.}.ufo"
```

Ya con la información en 5 máquinas de AWS podemos empezar a hacer los queries.

### Observaciones totales

Sacamos las observaciones totales de la base (antes de filtrar nada) con el script `obs_totales.sh`. Lo importante es usar el paralelismo para agrupar la información en cada máquina de AWS, para posteriormente sumar los totales en la máquina local:

```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); do wc -l \$file; done \
    | awk -F' ' '{ suma += \$1 } END { print suma }'" \
| awk '{ suma += $1 } END { print suma }'
```
```{r}
system('cat obs_totales.result', intern = T)
```

### Top 5 estados / top 5 estados por año

El segundo query consiste en obtener los 5 estados con la mayor cantidad de avistamientos. Lo hacemos mediante el script `top_5_estados.sh`. Como siempre, agrupamos antes de regresar la información a reducir.
```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); do awk -F' ' '{ print \$4 }' \$file | sort | uniq -c; done" \
| sed -e 's/ \+//' -e 's/[^ 0-9A-Z]//g' \
| egrep ' [A-Z][A-Z]$' \
| sort -k 2  \
| awk -F' ' '{ n[$2] += $1 } END { for(state in n) print state, n[state] }' \
| awk -F' ' '{ if($2 > 50) print }' \
| sort -nrk 2
| sed -n 1,5p
```
```{r}
read.table('top_5_estados.result', col.names=c('Estado','Avistamientos'))
```

Ahora veamos cómo se obtiene el top 5 de estados por año (`top_5_estados_anio.sh`). En este caso tuvimos que tener cuidado y rellenar los años a formato de 4 dígitos. En lugar de usar `sort` y `sed` o `head`, usamos `awk` para obtener el top 5 de cada año:
```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); \
      do \
        awk -F'\t' '{ if(NR != 1) print substr(\$1,1), \$4 }' \$file \
        | sed 's/\(^[0-9][0-9]\/\)\([0-9]\+\/\)\([0-9][0-9]\)/\3/' \
        | grep '^[0-9][0-9] [A-Z][A-Z]$' \
        | sort -t ' ' -k 2 -k 1 \
        | uniq -c \
        | sed -e 's/^ \+//'; \
      done" \
| awk -F' ' '{ count[$2,$3] += $1 } END { for(i in count) print substr(i,1,2), substr(i,4,2), count[i] }' \
| awk -F' ' '{if(match($1,/[0-1][0-9]/) > 0){$1 = 20$1}else{$1 = 19$1}; print}' \
| sort -k 1n,1 -k 3nr,3 \
| awk -F' ' '{if(NR==1 || yr!=$1){yr=$1; n=1}else{if(n<6){print; n++}}}'
```
```{r}
tail(read.table('top_5_estados_anio.result', col.names=c('Anio','Estado','Avistamientos')), 15)
```

### Rachas por estado y en el país

Calcular las rachas por estado fue un poco más complicado. El script general para hacerlo se llama `racha_por_estado.sh` y utiliza tres scripts para calcular la racha más larga de cada estado. El resultado final que buscamos es una tabla con Estado, la longitud de la racha más larga, la fecha de inicio de la racha y la fecha de fin de la racha.

```{shell}
#! /bin/zsh
./info_racha_por_estado.sh \
| ./info_racha_por_estado.R \
| ./sumariza_racha_por_estado.sh \
| awk '/[A-Z][A-Z]/ {if(NR == 1){ print "Estado\tNR\tRacha\tInicio\tFin"; print }else{ print }}'
```

La estructura anterior consiste en tres etapas. La primera, `info_racha_por_estado.sh` utiliza parallel para hacer un query básico a las máquinas de AWS, que contiene únicamente la información que necesitaremos. El resultado es una tabla con dos campos: estado (a dos letras) y fecha (AAAA-MM-DD). Para ello tuvimos que agregar los primeros dos términos de año y limpiar bastante la información. El punto de pasarla a este formato es el manejo fácil de fechas en R (paso dos).

```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); \
      do \
        awk -F'\t' '{ if(NR != 1) print \$4, \$1}' \$file \
        | grep '^[A-Z][A-Z] ' \
        | sed -e 's/ \([1-9]\)\// 0\1\//' \
            -e 's/\/\([0-9]\)\//\/0\1\//' \
        | awk -F' ' '\
            BEGIN {\
              OFS = \"\t\" \
            } \
            { \
              d = substr(\$2,1,2); \
              m = substr(\$2,4,2); \
              y = substr(\$2,7,2); \
              if(match(y,/[0-1][0-9]/) > 0){ \
                print \$1, \"20\" y \"-\" m \"-\" d; \
              }else{ \
                print \$1, \"19\" y \"-\" m \"-\" d; \
              } \
            } \
          '; \
      done" \
| grep -E '[A-Z]{2}.[0-9]{4}-[0-9]{2}-[0-9]{2}' \
| sort -k1 -k2
```

Ya con la información ordenada podemos pasar al segundo paso: el cálculo de las rachas. La dificultad en este punto es que tenemos un conflicto entre usar R y `awk`, debido a que R tiene un manejo de fechas muy fácil de usar, pero usar `for`s en R es _increíblemente_ lento. Lo que decidimos hacer fue calcular las diferencias de días entre una y otra observación (por estado) con `info_racha_por_estado.R` pero de manera vectorizada, que es razonablemente rápida, para posteriormente calcular las rachas en `awk` (paso tres):

```{r, eval=FALSE}
#! /usr/bin/env Rscript
require(dplyr, quietly = TRUE, warn.conflicts = FALSE)
f <- file('stdin')
open(f)

df <- read.table(f, header = F, sep = "\t", skipNul = T, stringsAsFactors = F)
colnames(df) <- c('state', 'chardate')
df$date <- as.Date(df$chardate)
df <- filter(df, !is.na(date)) %>%
  unique

df$dif <- 1
df$racha <- 1
df$inicio <- as.Date('0000-01-01')
df$fin <- as.Date('0000-01-01')
df$dif[2:nrow(df)] <- df$date[2:nrow(df)] - df$date[1:(nrow(df)-1)]
inicio <- df$date[1]
fin <- inicio

write.csv(df[c('state', 'date', 'dif', 'racha', 'inicio', 'fin')], file=stdout())
close(f)
```

Y calculamos las rachas:

```{shell}
#! /bin/zsh
sed 's/"//g' \
| awk -F',' '
  {
    if(NR == 1){
      inicio = $3
      fin = inicio
    }
    if($4 == 1){
      racha++
      fin = $3
    }else{
      rachas[$2,$1-1] = racha
      inicios[$2,$1-1] = inicio
      fines[$2,$1-1] = fin
      inicio = $3
      fin = inicio
      racha = 1
    }
  }
  END {
    OFS = "\t"
    for(i in rachas){
      print substr(i,1,2), substr(i,3), rachas[i], inicios[i], fines[i]
    }
  }
' \
| sort -k 1,1 -k 3nr,3 -k 4r,4 \
| awk 'BEGIN { state="" } {if(NR == 1 || state != $1){ print; state = $1 } }'
```
```{r}
x <- read.table('rachas_por_estado.result', header=T, stringsAsFactors = F)
x$NR <- as.numeric(sapply(x$NR, function(v) substr(v, 2, nchar(v))))
x
```

Cabe notar que algunos de los "estados" no son realmente estados (hay 68), pero debido a que la base no venía muy limpia, algunas provincias extranjeras se confunden con estados. Por el momento no tenemos una manera de filtrarlos más que corroborándolos en una lista de estados legítimos.

Ahora va la versión de todo el país. El código es muy similar, excepto por el hecho de que omitimos el estado, cosa que simplifica todo. Primero tenemos un script maestro `rachas_pais.sh` que llama a los otros tres:

```{shell}
#! /bin/zsh
./info_racha_pais.sh \
| ./info_racha_pais.R \
| ./sumariza_racha_pais.sh
```

Bajamos la información de AWS y la preparamos para leerla en R con `info_racha_pais.sh`:

```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); \
      do \
        awk -F'\t' '{ if(NR != 1) print \$4, \$1}' \$file \
        | grep '^[A-Z][A-Z] ' \
        | sed -e 's/ \([1-9]\)\// 0\1\//' \
              -e 's/\/\([0-9]\)\//\/0\1\//' \
        | awk -F' ' '\
            BEGIN {\
              OFS = \"\t\" \
            } \
            { \
              d = substr(\$2,1,2); \
              m = substr(\$2,4,2); \
              y = substr(\$2,7,2); \
              if(match(y,/[0-1][0-9]/) > 0){ \
                print \"20\" y \"-\" m \"-\" d; \
              }else{ \
                print \"19\" y \"-\" m \"-\" d; \
              } \
            } \
          '; \
      done" \
| grep -E '[0-9]{4}-[0-9]{2}-[0-9]{2}' \
| sort \
| uniq
```

Tratamos con las fechas en R con `info_racha_pais.R`:

```{r, eval=FALSE}
#! /usr/bin/env Rscript
require(dplyr, quietly = TRUE, warn.conflicts = FALSE)
f <- file('stdin')
open(f)

df <- read.table(f, header = F, sep = "\t", skipNul = T, stringsAsFactors = F)
colnames(df) <- c('chardate')
df$date <- as.Date(df$chardate)
df <- filter(df, !is.na(date)) %>%
  unique

df$dif <- 1
df$racha <- 1
df$inicio <- as.Date('0000-01-01')
df$fin <- as.Date('0000-01-01')
df$dif[2:nrow(df)] <- df$date[2:nrow(df)] - df$date[1:(nrow(df)-1)]
inicio <- df$date[1]
fin <- inicio

write.csv(df[c('date', 'dif', 'racha', 'inicio', 'fin')], file=stdout())
close(f)
```

Y finalmente calculamos las rachas con `sumariza_racha_pais.sh`. En este caso, regresamos un ranking de las rachas en lugar de solamente regresar la más larga.

```{shell}
#! /bin/zsh
sed '1d;s/"//g' \
| awk -F',' '
  {
    if(NR == 1){
      inicio = $2
      fin = inicio
    }
    if($3 == 1){
      racha++
      fin = $2
    }else{
      rachas[$1-1] = racha
      inicios[$1-1] = inicio
      fines[$1-1] = fin
      inicio = $2
      fin = inicio
      racha = 1
    }
  }
  END {
    OFS = "\t"
    for(i in rachas){
      print i, rachas[i], inicios[i], fines[i]
    }
  }
' \
| sort -k 2nr,2 -k 3r,3 \
| awk 'BEGIN { OFS="\t" } {if(NR == 1){ print "NR", "Racha", "Inicio", "Fin"; print }else{ print }}'
```
```{r}
read.table('rachas_pais.result', header = T) %>%
  arrange(desc(Racha), desc(Inicio), desc(Fin)) %>%
  head(25)
```

### Meses y días con más avistamientos

Calcular el mes con más avistamientos fue relativamente sencillo y lo podemos hacer puramente en shell (`top_meses.sh`). Lo único que hay que hacer es extraer el mes de cada avistamiento y usar un `sort | uniq -c`:

```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); \
      do \
        awk -F'\t' '{ if(NR != 1) print \$4, \$1}' \$file \
        | grep '^[A-Z][A-Z] ' \
        | sed -e 's/ \([1-9]\)\// 0\1\//' \
              -e 's/\/\([0-9]\)\//\/0\1\//' \
        | awk -F' ' '\
            BEGIN {\
              OFS = \"\t\" \
            } \
            { \
              print substr(\$2,4,2); \
            } \
          '; \
      done" \
| grep -E '(0[1-9])|(1[0-2])' \
| sort \
| uniq -c \
| awk '{ print $2, $1 }' \
> temp001.temp;

echo "ene,feb,mar,abr,may,jun,jul,ago,sep,oct,nov,dic" | tr ',' '\n' > temp002.temp;

paste temp002.temp temp001.temp | tr ' ' '\t' | sort -k3nr \
| awk -F'\t' 'BEGIN { OFS = "\t" } { if(NR == 1){ print "Ranking", "Mes", "MesNum", "Avistamientos"; print NR, $0}else{ print NR, $0}}'
```
```{r}
read.table('ranking_meses.result', header=T)
```

En realidad checar lo anterior para los días de la semana es prácticamente lo mismo (`top_dias_semana.sh`). La única diferencia consiste en que usaremos R (`dia_semana.R`) para extraer el día de la semana de una fecha dada:

```{shell}
#! /bin/zsh
parallel --nonall --progress --slf instancias_aws \
    "for file in \$(find ufo_data/*); \
      do \
        awk -F'\t' '{ if(NR != 1) print \$4, \$1}' \$file \
        | grep '^[A-Z][A-Z] ' \
        | sed -e 's/ \([1-9]\)\// 0\1\//' \
              -e 's/\/\([0-9]\)\//\/0\1\//' \
        | awk -F' ' '\
            BEGIN {\
              OFS = \"\t\" \
            } \
            { \
              d = substr(\$2,1,2); \
              m = substr(\$2,4,2); \
              y = substr(\$2,7,2); \
              if(match(y,/[0-1][0-9]/) > 0){ \
                print \"20\" y \"-\" m \"-\" d; \
              }else{ \
                print \"19\" y \"-\" m \"-\" d; \
              } \
            } \
          '; \
      done" \
| grep -E '([0-9]{4})-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])' \
| ./dia_semana.R \
| sort \
| uniq -c \
| sed 's/"//g' \
| sort -r \
| awk 'BEGIN { OFS = "\t" } NR == 1 {print "Ranking", "Numero", "Dia"} { print NR, $1, $2 }'
```

Donde el script de R es:

```{r,eval=FALSE}
#! /usr/bin/env Rscript
f <- file('stdin')
open(f)
while(length(line <- readLines(f, n = 1)) > 0){
  write.table(weekdays(as.Date(line)), file = stdout(), row.names = F, col.names = F)
}
close(f)
```
```{r}
read.table('ranking_dias.result', header=T)[c(1,3,2)]
```

### Gráficas de series de tiempo

Adicionalmente, hicimos un script en R (basado en `dplyr` y `txtplot`) para graficar la serie de tiempo de avistamientos anuales en un estado o en el país completo. Para ello, generamos la información mediante `info_plot.sh > plot.data`, donde `info_plot.sh` es idéntico a `info_racha_por_estado.sh` y posteriormente usamos el script `plot_ufo.R` para graficar:

```{r, eval=FALSE}
require(txtplot, warn.conflicts = F, quietly = T)
require(dplyr, warn.conflicts = F, quietly = T)

f <- file('stdin')
open(f)
cat('Proporciona un estado...\n')
state <- readLines(f, n=1)
df <- read.table('plot.data', col.names=c('Estado', 'Fecha'))
df$Fecha <- as.Date(df$Fecha)
if(state == "" || state == 'EUA' || state == 'todo'){
  cat("Graficando informacion de todo el pais...\n")
  df2 <- df %>%
    mutate(Anio=as.numeric(format(Fecha, "%Y"))) %>%
    group_by(Anio) %>%
    summarise(Avistamientos=as.numeric(n())) %>%
    mutate(FechaNum=1)
  df2$FechaNum[2:nrow(df2)] <- df2$Fecha[2:nrow(df2)] - df2$Fecha[1]
  head(df2)
  txtplot(df2$Anio, df2$Avistamientos, xlab='Anio', ylab='# en EUA')
}else{
  cat("Graficando informacion de ", state, "...\n", sep='')
  df2 <- df %>% filter(Estado==state) %>%
    mutate(Anio=as.numeric(format(Fecha, "%Y"))) %>%
    group_by(Anio) %>%
    summarise(Avistamientos=as.numeric(n())) %>%
    mutate(FechaNum=1)
  if(nrow(df2) == 0) stop("Estado invalido!")
  df2$FechaNum[2:nrow(df2)] <- df2$Fecha[2:nrow(df2)] - df2$Fecha[1]
  head(df2)
  txtplot(df2$Anio, df2$Avistamientos, xlab='Anio', ylab=paste('# en', state))
}
```

Podemos mandar el estado al script por medio de un pipe o bien esperar a que el programa nos pida un estado. Para graficar todo podemos simplemente dar <Enter> cuando nos pregunta qué estado queremos, o introducir 'EUA' o 'todo'. Elegimos graficar en ASCII usando el paquete `txtplot` con el fin de poder correr el código sin salir de la consola. Si quisiéramos graficar en `ggplot2` o con `base` en R sería muy fácil cambiar el código, pero habría que correrlo desde R, no como `Rscript`.

```{r}
cat(paste(system('echo "TX" | ./plot_ufo.R', intern=T), collapse='\n'))
cat(paste(system('echo "WA" | ./plot_ufo.R', intern=T), collapse='\n'))
cat(paste(system('echo "NY" | ./plot_ufo.R', intern=T), collapse='\n'))
cat(paste(system('echo "EUA" | ./plot_ufo.R', intern=T), collapse='\n'))
```






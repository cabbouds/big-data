<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GNU Parallel design</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rev="made" href="mailto:root@localhost" />
</head>

<body style="background-color: white">



<ul id="index">
  <li><a href="#Design-of-GNU-Parallel">Design of GNU Parallel</a>
    <ul>
      <li><a href="#One-file-program">One file program</a></li>
      <li><a href="#Old-Perl-style">Old Perl style</a></li>
      <li><a href="#Exponentially-back-off">Exponentially back off</a></li>
      <li><a href="#Shell-compatibility">Shell compatibility</a></li>
      <li><a href="#Job-slots">Job slots</a></li>
      <li><a href="#Rsync-protocol-version">Rsync protocol version</a></li>
      <li><a href="#Compression">Compression</a></li>
      <li><a href="#Wrapping">Wrapping</a></li>
      <li><a href="#Shell-shock">Shell shock</a></li>
      <li><a href="#Remote-Ctrl-C-and-standard-error-stderr">Remote Ctrl-C and standard error (stderr)</a></li>
      <li><a href="#Transferring-of-variables-and-functions">Transferring of variables and functions</a></li>
      <li><a href="#Base64-encode-bzip2">Base64 encode bzip2</a></li>
      <li><a href="#Which-shell-to-use">Which shell to use</a></li>
      <li><a href="#Quoting">Quoting</a></li>
      <li><a href="#pipepart-vs.---pipe">--pipepart vs. --pipe</a></li>
      <li><a href="#jobs-and---onall">--jobs and --onall</a></li>
      <li><a href="#Buffering-on-disk">Buffering on disk</a></li>
      <li><a href="#Disk-full">Disk full</a></li>
      <li><a href="#Perl-replacement-strings-and---rpl">Perl replacement strings, {= =}, and --rpl</a></li>
      <li><a href="#Test-suite">Test suite</a></li>
      <li><a href="#Median-run-time">Median run time</a></li>
      <li><a href="#Error-messages-and-warnings">Error messages and warnings</a></li>
      <li><a href="#Computation-of-load">Computation of load</a></li>
    </ul>
  </li>
  <li><a href="#Ideas-for-new-design">Ideas for new design</a>
    <ul>
      <li><a href="#Multiple-processes-working-together">Multiple processes working together</a></li>
      <li><a href="#Transferring-of-variables-and-functions-from-zsh">Transferring of variables and functions from zsh</a></li>
      <li><a href="#rrs-on-remote-using-a-perl-wrapper">--rrs on remote using a perl wrapper</a></li>
    </ul>
  </li>
  <li><a href="#Historical-decisions">Historical decisions</a>
    <ul>
      <li><a href="#tollef">--tollef</a></li>
      <li><a href="#Transferring-of-variables-and-functions1">Transferring of variables and functions</a></li>
    </ul>
  </li>
</ul>

<h1 id="Design-of-GNU-Parallel">Design of GNU Parallel</h1>

<p>This document describes design decisions made in the development of GNU <b>parallel</b> and the reasoning behind them. It will give an overview of why some of the code looks like it does, and help new maintainers understand the code better.</p>

<h2 id="One-file-program">One file program</h2>

<p>GNU <b>parallel</b> is a Perl script in a single file. It is object oriented, but contrary to normal Perl scripts each class is not in its own file. This is due to user experience: The goal is that in a pinch the user will be able to get GNU <b>parallel</b> working simply by copying a single file: No need messing around with environment variables like PERL5LIB.</p>

<h2 id="Old-Perl-style">Old Perl style</h2>

<p>GNU <b>parallel</b> uses some old, deprecated constructs. This is due to a goal of being able to run on old installations. Currently the target is CentOS 3.9 and Perl 5.8.0.</p>

<h2 id="Exponentially-back-off">Exponentially back off</h2>

<p>GNU <b>parallel</b> busy waits. This is because the reason why a job is not started may be due to load average, and thus it will not make sense to wait for a job to finish. Instead the load average must be checked again. Load average is not the only reason.</p>

<p>To not burn up too up too much CPU GNU <b>parallel</b> sleeps exponentially longer and longer if nothing happens, maxing out at 1 second.</p>

<h2 id="Shell-compatibility">Shell compatibility</h2>

<p>It is a goal to have GNU <b>parallel</b> work equally well in any shell. However, in practice GNU <b>parallel</b> is being developed in <b>bash</b> and thus testing in other shells is limited to reported bugs.</p>

<p>When an incompatibility is found there is often not an easy fix: Fixing the problem in <b>csh</b> often breaks it in <b>bash</b>. In these cases the fix is often to use a small Perl script and call that.</p>

<h2 id="Job-slots">Job slots</h2>

<p>The easiest way to explain what GNU <b>parallel</b> does is to assume that there are a number of job slots, and when a slot becomes available a job from the queue will be run in that slot. But originally GNU <b>parallel</b> did not model job slots in the code. Job slots have been added to make it possible to use {%} as a replacement string.</p>

<p>Job slots were added to the code in 20140522, but while the job sequence number can be computed in advance, the job slot can only be computed the moment a slot becomes available. So it has been implemented as a stack with lazy evaluation: Draw one from an empty stack and the stack is extended by one. When a job is done, push the available job slot back on the stack.</p>

<p>This implementation also means that if you use remote executions, you cannot assume that a given job slot will remain on the same remote server. This goes double since number of job slots can be adjusted on the fly (by giving <b>--jobs</b> a file name).</p>

<h2 id="Rsync-protocol-version">Rsync protocol version</h2>

<p><b>rsync</b> 3.1.x uses protocol 31 which is unsupported by version 2.5.7. That means that you cannot push a file to a remote system using <b>rsync</b> protocol 31, if the remote system uses 2.5.7. <b>rsync</b> does not automatically downgrade to protocol 30.</p>

<p>GNU <b>parallel</b> does not require protocol 31, so if the <b>rsync</b> version is &gt;= 3.1.0 then <b>--protocol 30</b> is added to force newer <b>rsync</b>s to talk to version 2.5.7.</p>

<h2 id="Compression">Compression</h2>

<p><b>--compress</b> compresses the data in the temporary files. This is a bit tricky because there should be no files to clean up if GNU <b>parallel</b> is killed by a power outage.</p>

<p>GNU <b>parallel</b> first selects a compress program. If the user has not selected one, the first of these that are in $PATH is used: <b>lzop pigz pxz gzip plzip pbzip2 lzma xz lzip bzip2</b>. They are sorted by speed on a 8 core machine.</p>

<p>Schematically the setup is as follows:</p>

<pre><code>  command started by parallel | compress &gt; tmpfile
  cattail tmpfile | uncompress | parallel</code></pre>

<p>The setup is duplicated for both standard output (stdout) and standard error (stderr).</p>

<p>GNU <b>parallel</b> pipes output from the command run into the compress program which saves to a tmpfile. GNU <b>parallel</b> records the pid of the compress program. At the same time a small perl script (called <b>cattail</b> above) is started: It basically does <b>cat</b> followed by <b>tail -f</b>, but it also removes the tmpfile as soon as the first byte is read, and it continously checks if the pid of the compress program is dead. If the compress program is dead, <b>cattail</b> reads the rest of tmpfile and exits.</p>

<p>As most compress programs write out a header when they start, the tmpfile in practice is unlinked after around 40 ms.</p>

<h2 id="Wrapping">Wrapping</h2>

<p>The command given by the user can be wrapped in multiple templates. Templates can be wrapped in other templates.</p>

<dl>

<dt id="shellquote">--shellquote</dt>
<dd>

<p>echo &lt;&lt;shell double quoted input&gt;&gt;</p>

</dd>
<dt id="nice-pri">--nice <i>pri</i></dt>
<dd>

<p>\nice -n <i>pri</i> $shell -c &lt;&lt;shell quoted input&gt;&gt;</p>

<p>The \ is needed to avoid using the builtin nice command, which does not support -n in <b>tcsh</b>. <b>$shell -c</b> is needed to nice composed commands command.</p>

</dd>
<dt id="cat">--cat</dt>
<dd>

<p>(cat &gt; {}; &lt;&lt;input&gt;&gt; {}; perl -e &#39;$bash=shift; $csh=shift; for(@ARGV) {unlink;rmdir;} if($bash=~s/h//) {exit$bash;} exit$csh;&#39; &quot;$?h&quot; &quot;$status&quot; {});</p>

<p>{} is really just a tmpfile. The Perl script saves the exit value, unlinks the tmpfile, and returns the exit value - no matter if the shell is <b>bash</b> (using $?) or <b>*csh</b> (using $status).</p>

</dd>
<dt id="fifo">--fifo</dt>
<dd>

<p>(mkfifo {}; (&lt;&lt;input&gt;&gt; {};) &amp; _PID=$!; cat &gt; {}; wait $_PID; perl -e &#39;$bash=shift; $csh=shift; for(@ARGV) {unlink;rmdir;} if($bash=~s/h//) {exit$bash;} exit$csh;&#39; &quot;$?h&quot; &quot;$status&quot; {});</p>

<p><b>wait $_PID</b> makes sure the exit value is from that PID. This makes it incompatible with <b>*csh</b>. The Perl script is the same as from <b>--cat</b>.</p>

</dd>
<dt id="sshlogin-sln">--sshlogin <i>sln</i></dt>
<dd>

<p>ssh <i>sln</i> &lt;&lt;shell quoted input&gt;&gt;</p>

</dd>
<dt id="transfer">--transfer</dt>
<dd>

<p>( ssh <i>sln</i> mkdir -p ./<i>workdir</i>;rsync --protocol 30 -rlDzR -essh ./{} <i>sln</i>:./<i>workdir</i> ); &lt;&lt;input&gt;&gt;</p>

<p>Read about <b>--protocol 30</b> in the section <b>Rsync protocol version</b>.</p>

</dd>
<dt id="basefile">--basefile</dt>
<dd>

<p>&lt;&lt;todo&gt;&gt;</p>

</dd>
<dt id="return-file">--return <i>file</i></dt>
<dd>

<p>&lt;&lt;input&gt;&gt;; _EXIT_status=$?; mkdir -p <i>workdir</i>; rsync --protocol 30 --rsync-path=cd\ ./<i>workdir</i>\;\ rsync -rlDzR -essh <i>sln</i>:./<i>file</i> ./<i>workdir</i>; exit $_EXIT_status;</p>

<p>The <b>--rsync-path=cd ...</b> is needed because old versions of <b>rsync</b> do not support <b>--no-implied-dirs</b>.</p>

<p>The <b>$_EXIT_status</b> trick is to postpone the exit value. This makes it incompatible with <b>*csh</b> and should be fixed in the future. Maybe a wrapping &#39;sh -c&#39; is enough?</p>

</dd>
<dt id="cleanup">--cleanup</dt>
<dd>

<p>&lt;&lt;input&gt;&gt; _EXIT_status=$?; &lt;&lt;return&gt;&gt;</p>

<p>ssh <i>sln</i> \(rm\ -f\ ./<i>workdir</i>/{}\;\ rmdir\ ./<i>workdir</i>\ \&gt;\&amp;/dev/null\;\); exit $_EXIT_status;</p>

<p><b>$_EXIT_status</b>: see <b>--return</b> above.</p>

</dd>
<dt id="pipe">--pipe</dt>
<dd>

<p>sh -c &#39;dd bs=1 count=1 of=<i>tmpfile</i> 2&gt;/dev/null&#39;; test ! -s &quot;<i>tmpfile</i>&quot; &amp;&amp; rm -f &quot;<i>tmpfile</i>&quot; &amp;&amp; exec true; (cat <i>tmpfile</i>; rm <i>tmpfile</i>; cat - ) | ( &lt;&lt;input&gt;&gt; );</p>

<p>This small wrapper makes sure that &lt;&lt;input&gt;&gt; will never be run if there is no data. <b>sh -c</b> is needed to hide stderr if the user&#39;s shell is <b>csh</b> (which cannot hide stderr).</p>

</dd>
<dt id="tmux">--tmux</dt>
<dd>

<p>mkfifo <i>tmpfile</i>; tmux new-session -s p<i>PID</i> -d -n &lt;&lt;shell quoted input&gt;&gt; \(&lt;&lt;shell quoted input&gt;&gt;\)\;\ perl\ -e\ \&#39;while\(\$t++\&lt;3\)\{\ print\ \$ARGV\[0\],\&quot;\\n\&quot;\ \}\&#39;\ \$\?h/\$status/255\ \&gt;\&gt;\ <i>tmpfile</i>\&amp;echo\ &lt;&lt;shell double quoted input&gt;&gt;\;echo\ \Job\ finished\ at:\ \`date\`\;sleep\ 10; exec perl -e &#39;$/=&quot;/&quot;;$_=&lt;&gt;;$c=&lt;&gt;;unlink $ARGV; /(\d+)h/ and exit($1);exit$c&#39; <i>tmpfile</i></p>

<p>The input is used as the name of the windows in <b>tmux</b>. When the job inside <b>tmux</b> finishes, the exit value is printed to a fifo. This fifo is opened by perl outside <b>tmux</b>, and perl then removes the fifo (but keeping it open). Perl blocks until the first value is read from the fifo, and this value is used as exit value.</p>

<p>To make it compatible with <b>csh</b> and <b>bash</b> the exit value is printed as: $?h/$status/255 and this is parsed by perl.</p>

<p>There is a bug that makes it necessary to print the exit value 3 times. Works in <b>csh</b>.</p>

</dd>
</dl>

<p>The ordering of the wrapping is important:</p>

<ul>

<li><p><b>--nice</b>/<b>--cat</b>/<b>--fifo</b> should be done on the remote machine</p>

</li>
<li><p><b>--pipepart</b>/<b>--pipe</b> should be done on the local machine inside <b>--tmux</b></p>

</li>
</ul>

<h2 id="Shell-shock">Shell shock</h2>

<p>The shell shock bug in <b>bash</b> did not affect GNU <b>parallel</b>, but the solutions did. <b>bash</b> first introduced functions in variables named: <i>BASH_FUNC_myfunc()</i> and later changed that to <i>BASH_FUNC_myfunc%%</i>. When transferring functions GNU <b>parallel</b> reads off the function and changes that into a function definition, which is copied to the remote system and executed before the actual command is executed. Therefore GNU <b>parallel</b> needs to know how to read the function.</p>

<p>From version 20150122 GNU <b>parallel</b> tries both the ()-version and the %%-version, and the function definition works on both pre- and post-shellshock versions of <b>bash</b>.</p>

<h2 id="Remote-Ctrl-C-and-standard-error-stderr">Remote Ctrl-C and standard error (stderr)</h2>

<p>If the user presses Ctrl-C the user expect jobs to stop. This works out of the box if the jobs are run locally. Unfortunately it is not so simple if the jobs are run remotely.</p>

<p>If remote jobs are run in a tty using <b>ssh -tt</b>, then Ctrl-C works, but all output to standard error (stderr) is sent to standard output (stdout). This is not what the user expects.</p>

<p>If remote jobs are run without a tty using <b>ssh</b> (without <b>-tt</b>), then output to standard error (stderr) is kept on stderr, but Ctrl-C does not kill remote jobs. This is not what the user expects.</p>

<p>So what is needed is a way to have both. It seems the reason why Ctrl-C does not kill the remote jobs is because the shell does not propagate the hang-up signal from <b>sshd</b>. But when <b>sshd</b> dies, the parent of the login shell becomes <b>init</b> (process id 1). So by exec&#39;ing a Perl wrapper to monitor the parent pid and kill the child if the parent pid becomes 1, then Ctrl-C works and stderr is kept on stderr. The wrapper looks like this:</p>

<pre><code>    $SIG{CHLD} = sub { $done = 1; };
    $pid = fork;
    unless($pid) {
        # Make own process group to be able to kill HUP it later
        setpgrp;
        exec $ENV{SHELL}, &quot;-c&quot;, ($bashfunc.&quot;@ARGV&quot;);
        die &quot;exec: $!\n&quot;;
    }
    do {
        # Parent is not init (ppid=1), so sshd is alive
        # Exponential sleep up to 1 sec
        $s = $s &lt; 1 ? 0.001 + $s * 1.03 : $s;
        select(undef, undef, undef, $s);
    } until ($done || getppid == 1);
    # Kill HUP the process group if job not done
    kill(SIGHUP, -${pid}) unless $done;
    wait;
    exit ($?&amp;127 ? 128+($?&amp;127) : 1+$?&gt;&gt;8)</code></pre>

<h2 id="Transferring-of-variables-and-functions">Transferring of variables and functions</h2>

<p>Transferring of variables and functions given by <b>-env</b> is done by running a Perl script remotely that calls the actual command. The Perl script sets $ENV{variable} to the correct value before exec&#39;ing the a shell that runs the function definition followed by the actual command.</p>

<p><b>env_parallel</b> (mentioned in the man page) copies the full current environment into the environment variable <b>parallel_bash_environment</b>. This variable is picked up by GNU <b>parallel</b> and used to create the Perl script mentioned above.</p>

<h2 id="Base64-encode-bzip2">Base64 encode bzip2</h2>

<p><b>csh</b> limits words of commands to 1024 chars. This is often too little when GNU <b>parallel</b> encodes environment variables and wraps the command with different templates. All of these are combined and quoted into one single word, which often is longer than 1024 chars.</p>

<p>When the line to run is &gt; 1000 chars, GNU <b>parallel</b> therefore encodes the line to run. The encoding <b>bzip2</b>s the line to run, converts this to base64, splits the base64 into 1000 char blocks (so <b>csh</b> does not fail), and prepends it with this Perl script that decodes, decompresses and <b>eval</b>s the line.</p>

<pre><code>    @GNU_Parallel=(&quot;use&quot;,&quot;IPC::Open3;&quot;,&quot;use&quot;,&quot;MIME::Base64&quot;);
    eval &quot;@GNU_Parallel&quot;;

    $SIG{CHLD}=&quot;IGNORE&quot;;
    # Search for bzip2. Not found =&gt; use default path
    my $zip = (grep { -x $_ } &quot;/usr/local/bin/bzip2&quot;)[0] || &quot;bzip2&quot;;
    # $in = stdin on $zip, $out = stdout from $zip
    my($in, $out,$eval);
    open3($in,$out,&quot;&gt;&amp;STDERR&quot;,$zip,&quot;-dc&quot;);
    if(my $perlpid = fork) {
        close $in;
        $eval = join &quot;&quot;, &lt;$out&gt;;
        close $out;
    } else {
        close $out;
        # Pipe decoded base64 into &#39;bzip2 -dc&#39;
        print $in (decode_base64(join&quot;&quot;,@ARGV));
        close $in;
        exit;
    }
    wait;
    eval $eval;</code></pre>

<p>Perl and <b>bzip2</b> must be installed on the remote system, but a small test showed that <b>bzip2</b> is installed by default on all platforms that runs GNU <b>parallel</b>, so this is not a big problem.</p>

<p>The added bonus of this is that much bigger environments can now be transferred as they will be below <b>bash</b>&#39;s limit of 131072 chars.</p>

<h2 id="Which-shell-to-use">Which shell to use</h2>

<p>Different shells behave differently. A command that works in <b>tcsh</b> may not work in <b>bash</b>. It is therefore important that the correct shell is used when GNU <b>parallel</b> executes commands.</p>

<p>GNU <b>parallel</b> tries hard to use the right shell. If GNU <b>parallel</b> is called from <b>tcsh</b> it will use <b>tcsh</b>. If it is called from <b>bash</b> it will use <b>bash</b>. It does this by looking at the (grand*)parent process: If the (grand*)parent process is a shell, use this shell; otherwise look at the parent of this (grand*)parent. If none of the (grand*)parents are shells, then $SHELL is used.</p>

<p>This will do the right thing if called from:</p>

<ul>

<li><p>an interactive shell</p>

</li>
<li><p>a shell script</p>

</li>
<li><p>a Perl script in `` or using <b>system</b> if called as a single string.</p>

</li>
</ul>

<p>While these cover most cases, there are situations where it will fail:</p>

<pre><code>  #!/usr/bin/perl

  system(&quot;parallel&quot;,&#39;setenv a {}; echo $a&#39;,&quot;:::&quot;,2);</code></pre>

<p>Here it depends on which shell is used to call the Perl script. If the Perl script is called from <b>tcsh</b> it will work just fine, but if it is called from <b>bash</b> it will fail, because the command <b>setenv</b> is not known to <b>bash</b>.</p>

<h2 id="Quoting">Quoting</h2>

<p>Quoting is kept simple: Use \ for all special chars and &#39; for newline. Whether a char is special depends on the shell and the context. Luckily quoting a bit too many does not break things.</p>

<p>It is fast, but had the distinct disadvantage that if a string needs to be quoted multiple times, the \&#39;s double every time - increasing the string length exponentially.</p>

<h2 id="pipepart-vs.---pipe">--pipepart vs. --pipe</h2>

<p>While <b>--pipe</b> and <b>--pipepart</b> look much the same to the user, they are implemented very differently.</p>

<p>With <b>--pipe</b> GNU <b>parallel</b> reads the blocks from standard input (stdin), which is then given to the command on standard input (stdin); so every block is being processed by GNU <b>parallel</b> itself. This is the reason why <b>--pipe</b> maxes out at around 100 MB/sec.</p>

<p><b>--pipepart</b>, on the other hand, first identifies at which byte positions blocks start and how long they are. It does that by seeking into the file by the size of a block and then reading until it meets end of a block. The seeking explains why GNU <b>parallel</b> does not know the line number and why <b>-L/-l</b> and <b>-N</b> do not work.</p>

<p>With a reasonable block and file size this seeking is often more than 1000 faster than reading the full file. The byte positions are then given to a small script that reads from position X to Y and sends output to standard output (stdout). This small script is prepended to the command and the full command is executed just as if GNU <b>parallel</b> had been in its normal mode. The script looks like this:</p>

<pre><code>  &lt; file perl -e &#39;while(@ARGV) { 
     sysseek(STDIN,shift,0) || die;
     $left = shift;
     while($read = sysread(STDIN,$buf, ($left &gt; 32768 ? 32768 : $left))){
       $left -= $read; syswrite(STDOUT,$buf);
     }
  }&#39; startbyte length_in_bytes</code></pre>

<p>It delivers 1 GB/s per core.</p>

<p>Instead of the script <b>dd</b> was tried, but many versions of <b>dd</b> do not support reading from one byte to another and might cause partial data. See this for a surprising example:</p>

<pre><code>  yes | dd bs=1024k count=10 | wc</code></pre>

<h2 id="jobs-and---onall">--jobs and --onall</h2>

<p>When running the same commands on many servers what should <b>--jobs</b> signify? Is it the number of servers to run on in parallel? Is it the number of jobs run in parallel on each server?</p>

<p>GNU <b>parallel</b> lets <b>--jobs</b> represent the number of servers to run on in parallel. This is to make it possible to run a sequence of commands (that cannot be parallelized) on each server, but run the same sequence on multiple servers.</p>

<h2 id="Buffering-on-disk">Buffering on disk</h2>

<p>GNU <b>parallel</b> buffers on disk in $TMPDIR using files, that are removed as soon as they are created, but which are kept open. So even if GNU <b>parallel</b> is killed by a power outage, there will be no files to clean up afterwards. Another advantage is that the file system is aware that these files will be lost in case of a crash, so it does not need to sync them to disk.</p>

<p>It gives the odd situation that a disk can be fully used, but there are no visible files on it.</p>

<h2 id="Disk-full">Disk full</h2>

<p>GNU <b>parallel</b> buffers on disk. If the disk is full data may be lost. To check if the disk is full GNU <b>parallel</b> writes a 8193 byte file when a job finishes. If this file is written successfully, it is removed immediately. If it is not written successfully, the disk is full. The size 8193 was chosen because 8192 gave wrong result on some file systems, whereas 8193 did the correct thing on all tested filesystems.</p>

<h2 id="Perl-replacement-strings-and---rpl">Perl replacement strings, {= =}, and --rpl</h2>

<p>The shorthands for replacement strings make a command look more cryptic. Different users will need different replacement strings. Instead of inventing more shorthands you get more more flexible replacement strings if they can be programmed by the user.</p>

<p>The language Perl was chosen because GNU <b>parallel</b> is written in Perl and it was easy and reasonably fast to run the code given by the user.</p>

<p>If a user needs the same programmed replacement string again and again, the user may want to make his own shorthand for it. This is what <b>--rpl</b> is for. It works so well, that even GNU <b>parallel</b>&#39;s own shorthands are implemented using <b>--rpl</b>.</p>

<p>In Perl code the bigrams {= and =} rarely exist. They look like a matching pair and can be entered on all keyboards. This made them good candidates for enclosing the Perl expression in the replacement strings. Another candidate ,, and ,, was rejected because they do not look like a matching pair. <b>--parens</b> was made, so that the users can still use ,, and ,, if they like: <b>--parens ,,,,</b></p>

<p>Internally, however, the {= and =} are replaced by \257&lt; and \257&gt;. This is to make it simple to make regular expressions: \257 is disallowed on the command line, so when that is matched in a regular expression, it is known that this is a replacement string.</p>

<h2 id="Test-suite">Test suite</h2>

<p>GNU <b>parallel</b> uses its own testing framework. This is mostly due to historical reasons. It deals reasonably well with tests that are dependent on how long a given test runs (e.g. more than 10 secs is a pass, but less is a fail). It parallelizes most tests, but it is easy to force a test to run as the single test (which may be important for timing issues). It deals reasonably well with tests that fail intermittently. It detects which tests failed and pushes these to the top, so when running the test suite again, the tests that failed most recently are run first.</p>

<p>If GNU <b>parallel</b> should adopt a real testing framework then those elements would be important.</p>

<p>Since many tests are dependent on which hardware it is running on, these tests break when run on a different hardware than what the test was written for.</p>

<p>When most bugs are fixed a test is added, so this bug will not reappear. It is, however, sometimes hard to create the environment in which the bug shows up - especially if the bug only shows up sometimes. One of the harder problems was to make a machine start swapping without forcing it to its knees.</p>

<h2 id="Median-run-time">Median run time</h2>

<p>Using a percentage for <b>--timeout</b> causes GNU <b>parallel</b> to compute the median run time of a job. The median is a better indicator of the expected run time than average, because there will often be outliers taking way longer than the normal run time.</p>

<p>To avoid keeping all run times in memory, an implementation of remedian was made (Rousseeuw et al).</p>

<h2 id="Error-messages-and-warnings">Error messages and warnings</h2>

<p>Error messages like: ERROR, Not found, and 42 are not very helpful. GNU <b>parallel</b> strives to inform the user:</p>

<ul>

<li><p>What went wrong?</p>

</li>
<li><p>Why did it go wrong?</p>

</li>
<li><p>What can be done about it?</p>

</li>
</ul>

<p>Unfortunately it is not always possible to predict the root cause of the error.</p>

<h2 id="Computation-of-load">Computation of load</h2>

<p>Contrary to the obvious <b>--load</b> does not use load average. This is due to load average rising too slowly. Instead it uses <b>ps</b> to list the number of jobs in running or blocked state (state D, O or R). This gives an instant load.</p>

<p>As remote calculation of load can be slow, a process is spawned to run <b>ps</b> and put the result in a file, which is then used next time.</p>

<h1 id="Ideas-for-new-design">Ideas for new design</h1>

<h2 id="Multiple-processes-working-together">Multiple processes working together</h2>

<p>Open3 is slow. Printing is slow. It would be good if they did not tie up ressources, but were run in separate threads.</p>

<h2 id="Transferring-of-variables-and-functions-from-zsh">Transferring of variables and functions from zsh</h2>

<p>Transferring Bash functions to remote zsh works. Can parallel_bash_environment be used to import zsh functions?</p>

<h2 id="rrs-on-remote-using-a-perl-wrapper">--rrs on remote using a perl wrapper</h2>

<p>... | perl -pe &#39;$/=$recend$recstart;BEGIN{ if(substr($_) eq $recstart) substr($_)=&quot;&quot; } eof and substr($_) eq $recend) substr($_)=&quot;&quot;</p>

<p>It ought to be possible to write a filter that removed rec sep on the fly instead of inside GNU <b>parallel</b>. This could then use more cpus.</p>

<p>Will that require 2x record size memory?</p>

<p>Will that require 2x block size memory?</p>

<h1 id="Historical-decisions">Historical decisions</h1>

<h2 id="tollef">--tollef</h2>

<p>You can read about the history of GNU <b>parallel</b> on https://www.gnu.org/software/parallel/history.html</p>

<p><b>--tollef</b> was included to make GNU <b>parallel</b> switch compatible with the parallel from moreutils (which is made by Tollef Fog Heen). This was done so that users of that parallel easily could port their use to GNU <b>parallel</b>: Simply set <b>PARALLEL=&quot;--tollef&quot;</b> and that would be it.</p>

<p>But several distributions chose to make <b>--tollef</b> global (by putting it into /etc/parallel/config), and that caused much confusion when people tried out the examples from GNU <b>parallel</b>&#39;s man page and these did not work. The users became frustrated because the distribution did not make it clear to them that it has made <b>--tollef</b> global.</p>

<p>So to lessen the frustration and the resulting support, <b>--tollef</b> was obsoleted 20130222 and removed one year later.</p>

<h2 id="Transferring-of-variables-and-functions1">Transferring of variables and functions</h2>

<p>Until 20150122 variables and functions were transferred by looking at $SHELL to see whether the shell was a <b>*csh</b> shell. If so the variables would be set using <b>setenv</b>. Otherwise they would be set using <b>=</b>. The caused the content of the variable to be repeated:</p>

<p>echo $SHELL | grep &quot;/t\{0,1\}csh&quot; &gt; /dev/null &amp;&amp; setenv VAR foo || export VAR=foo</p>


</body>

</html>


